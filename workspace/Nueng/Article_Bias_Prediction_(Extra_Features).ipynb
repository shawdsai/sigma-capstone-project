{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Oluody0skGA"
   },
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D8dXm6RYtVuW",
    "outputId": "71191c0e-5218-464e-d171-217eceb92e83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0L_qmYqxBbR0",
    "outputId": "403b9642-6caa-47ea-dcc3-d2d829103b93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Article-Bias-Prediction'...\n",
      "remote: Enumerating objects: 37585, done.\u001b[K\n",
      "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
      "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
      "remote: Total 37585 (delta 0), reused 0 (delta 0), pack-reused 37581 (from 1)\u001b[K\n",
      "Receiving objects: 100% (37585/37585), 127.14 MiB | 14.47 MiB/s, done.\n",
      "Resolving deltas: 100% (8/8), done.\n",
      "Updating files: 100% (37563/37563), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ramybaly/Article-Bias-Prediction.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tYkLCmgLDc12",
    "outputId": "ca20addc-8d08-464c-f5d2-f52000d29bb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Article-Bias-Prediction\n"
     ]
    }
   ],
   "source": [
    "cd Article-Bias-Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FGgwZtJB-pMK",
    "outputId": "802fb90b-90f4-40e8-beae-c9cbb23cfc98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 37554 articles.\n",
      "{'topic': 'coronavirus', 'source': 'The Guardian', 'bias': 0, 'url': 'https://www.theguardian.com/world/2020/mar/26/us-coronavirus-stimulus-all-you-need-to-know', 'title': 'US coronavirus stimulus checks: are you eligible and how much will you get?', 'date': '2020-03-26', 'authors': 'Lauren Aratani', 'content': \"Most taxpayers will get a check from the $ 2tn economic package , the largest in US history\\nUS coronavirus stimulus checks : are you eligible and how much will you get ?\\nThe US has agreed on a $ 2tn stimulus package , the largest economic stimulus in US history , in response to the economic impacts of Covid-19 . While corporations will be the biggest recipients of the bailout , some of that money will be paid directly to Americans hit by the pandemic .\\nMost taxpayers will get a check in the mail , while those directly affected by the economic effects of Covid-19 are slated to receive robust government support .\\nHere ’ s what we know so far about how the new stimulus package will directly affect Americans in the coming weeks :\\nCongress will spend about $ 250bn for checks up to $ 1,200 per person that will go directly to taxpayers .\\nTo be eligible for the full amount , a person ’ s most recently filed tax return must show that they made $ 75,000 or under . For couples , who can receive a maximum of $ 2,400 , the cutoff is $ 150,000 .\\nIf a person makes more than $ 75,000 , the amount given goes down incrementally by $ 5 for every $ 100 increase in salary . So a person who makes $ 85,000 would get $ 700 while a person who makes $ 95,000 would get $ 200 .\\nIf a person makes above $ 99,000 , or a couple makes above $ 198,000 , no check will be given .\\nThere are some requirements and exceptions . Those getting a check must be living and working in the US and have a valid social security number . If a person is listed as a dependent on their parents ’ tax return – the case for many college students – they are excluded .\\nThe Tax Foundation , a DC-based thinktank , estimates that 93.6 % of Americans will be eligible for a check coming from the stimulus package .\\nTaxpayers will be given $ 500 per child 16 or under listed as a dependent on their latest tax return .\\nThat is still unclear . Experts say that Americans will probably not get the money until May , given how long it takes the Internal Revenue Service ( IRS ) to send out checks . The treasury secretary , Steven Mnuchin , said those whose bank account information is already with the IRS from previous tax returns can expect a payment “ within three weeks ” .\\nFor those who still need to give the IRS their bank account information , a web-based portal will be opening soon to allow people to provide the IRS with their information . The IRS says this is to make sure people get their checks immediately , instead of sending them through the mail .\\nThis is not the first time the government has sent checks to Americans . The federal government gave up to $ 300 in 2001 and $ 600 in 2008 to taxpayers who met a certain income bracket to similarly stimulate the economy .\\nWhat if a person did not file their 2018 or 2019 return ?\\nThe IRS recommends people file their 2018 or 2019 tax return as soon as possible to get the payment . A check will be sent to any qualified person so long as they file their return within 2020 .\\nA person may still be eligible even if they do not file the taxes . The IRS says people who typically do not file taxes – low-income taxpayers and some veterans – will need to file a “ simple tax return ” in order to get the payment . Social security beneficiaries will not need to fill out the tax return , even if they do not file their taxes , to get the payment .\\nYes , specifically the bill will increase unemployment insurance by $ 600 for 13 weeks – about four months – for every person , added to the existing unemployment compensation a person gets from their state ’ s program .\\nThe length and amount of compensation varies from state to state . A majority of states providing a maximum of 26 weeks of compensation , while average weekly compensation ranges from 20 % of a person ’ s wage to just over 50 % .\\nWhile unemployment insurance typically does not cover people who are self-employed – freelancers , contractors and gig workers – the bill comes with a “ pandemic unemployment assistance ” measure that will extend insurance to those workers .\\nPlay Video 2:25 'The universe is collapsing ' : Bernie Sanders mocks Republicans over coronavirus aid – video\\nNot in this bill . Earlier last week , Donald Trump signed the Families First Coronavirus Response Act , a bill worth about $ 100bn meant to expand paid sick leave and emergency paid leave , but it came with major loopholes . Companies with over 500 employees were not mentioned in the bill , while companies with under 50 employees can apply for exemptions .\\nThe bill mandates 10 days of fully paid sick leave for employees of companies with 500 employees or less . Parents of those companies affected by their children ’ s school closing and those leaving for medical reasons can get 12 weeks of pay at 67 % of their salary .\\nWill there be anything to offset healthcare costs related to Covid-19 ?\\nAgain , not in this bill . The Families First Coronavirus Response Act included a measure that mandated all Covid-19 testing is free , but treatment for any symptoms ( there is currently no cure for the illness ) still comes at a cost .\\nA few states have reopened enrollment for their health insurance programs to allow those concerned about costs to enroll , and three major health insurance companies said they will be waiving any high costs for treatment , but there are still stories of people getting bills for as much as $ 34,000 to cover treatment of the virus ’ s symptoms .\", 'content_original': \"Most taxpayers will get a check from the $2tn economic package, the largest in US history\\n\\nUS coronavirus stimulus checks: are you eligible and how much will you get?\\n\\nThe US has agreed on a $2tn stimulus package, the largest economic stimulus in US history, in response to the economic impacts of Covid-19. While corporations will be the biggest recipients of the bailout, some of that money will be paid directly to Americans hit by the pandemic.\\n\\nMost taxpayers will get a check in the mail, while those directly affected by the economic effects of Covid-19 are slated to receive robust government support.\\n\\nHere’s what we know so far about how the new stimulus package will directly affect Americans in the coming weeks:\\n\\nWho’s eligible for a check from the government?\\n\\nCongress will spend about $250bn for checks up to $1,200 per person that will go directly to taxpayers.\\n\\nTo be eligible for the full amount, a person’s most recently filed tax return must show that they made $75,000 or under. For couples, who can receive a maximum of $2,400, the cutoff is $150,000.\\n\\nIf a person makes more than $75,000, the amount given goes down incrementally by $5 for every $100 increase in salary. So a person who makes $85,000 would get $700 while a person who makes $95,000 would get $200.\\n\\nIf a person makes above $99,000, or a couple makes above $198,000, no check will be given.\\n\\nThere are some requirements and exceptions. Those getting a check must be living and working in the US and have a valid social security number. If a person is listed as a dependent on their parents’ tax return – the case for many college students – they are excluded.\\n\\nThe Tax Foundation, a DC-based thinktank, estimates that 93.6% of Americans will be eligible for a check coming from the stimulus package.\\n\\nWhat about parents?\\n\\nTaxpayers will be given $500 per child 16 or under listed as a dependent on their latest tax return.\\n\\nWhen will I get this money?\\n\\nThat is still unclear. Experts say that Americans will probably not get the money until May, given how long it takes the Internal Revenue Service (IRS) to send out checks. The treasury secretary, Steven Mnuchin, said those whose bank account information is already with the IRS from previous tax returns can expect a payment “within three weeks”.\\n\\nFor those who still need to give the IRS their bank account information, a web-based portal will be opening soon to allow people to provide the IRS with their information. The IRS says this is to make sure people get their checks immediately, instead of sending them through the mail.\\n\\nThis is not the first time the government has sent checks to Americans. The federal government gave up to $300 in 2001 and $600 in 2008 to taxpayers who met a certain income bracket to similarly stimulate the economy.\\n\\nWhat if a person did not file their 2018 or 2019 return?\\n\\nThe IRS recommends people file their 2018 or 2019 tax return as soon as possible to get the payment. A check will be sent to any qualified person so long as they file their return within 2020.\\n\\nWhat if a person does not file their taxes?\\n\\nA person may still be eligible even if they do not file the taxes. The IRS says people who typically do not file taxes – low-income taxpayers and some veterans – will need to file a “simple tax return” in order to get the payment. Social security beneficiaries will not need to fill out the tax return, even if they do not file their taxes, to get the payment.\\n\\nDoes the package help unemployed workers?\\n\\nYes, specifically the bill will increase unemployment insurance by $600 for 13 weeks – about four months – for every person, added to the existing unemployment compensation a person gets from their state’s program.\\n\\nThe length and amount of compensation varies from state to state. A majority of states providing a maximum of 26 weeks of compensation, while average weekly compensation ranges from 20% of a person’s wage to just over 50%.\\n\\nWhat about freelance and gig workers?\\n\\nWhile unemployment insurance typically does not cover people who are self-employed – freelancers, contractors and gig workers – the bill comes with a “pandemic unemployment assistance” measure that will extend insurance to those workers.\\n\\nPlay Video 2:25 'The universe is collapsing': Bernie Sanders mocks Republicans over coronavirus aid – video\\n\\nHave there been any changes to paid leave?\\n\\nNot in this bill. Earlier last week, Donald Trump signed the Families First Coronavirus Response Act, a bill worth about $100bn meant to expand paid sick leave and emergency paid leave, but it came with major loopholes. Companies with over 500 employees were not mentioned in the bill, while companies with under 50 employees can apply for exemptions.\\n\\nThe bill mandates 10 days of fully paid sick leave for employees of companies with 500 employees or less. Parents of those companies affected by their children’s school closing and those leaving for medical reasons can get 12 weeks of pay at 67% of their salary.\\n\\nWill there be anything to offset healthcare costs related to Covid-19?\\n\\nAgain, not in this bill. The Families First Coronavirus Response Act included a measure that mandated all Covid-19 testing is free, but treatment for any symptoms (there is currently no cure for the illness) still comes at a cost.\\n\\nA few states have reopened enrollment for their health insurance programs to allow those concerned about costs to enroll, and three major health insurance companies said they will be waiving any high costs for treatment, but there are still stories of people getting bills for as much as $34,000 to cover treatment of the virus’s symptoms.\", 'source_url': 'www.theguardian.com', 'bias_text': 'left', 'ID': 'IBgtqEHUEcjyMriR'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Path to the jsons directory\n",
    "jsons_dir = \"./data/jsons/\"\n",
    "\n",
    "# Load all JSON articles into a dictionary\n",
    "articles = {}\n",
    "for filename in os.listdir(jsons_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(jsons_dir, filename)\n",
    "        with open(file_path, \"r\", encoding = \"utf-8\") as f:\n",
    "            article = json.load(f)\n",
    "            articles[article[\"ID\"]] = article\n",
    "\n",
    "# Print the number of articles loaded\n",
    "print(f\"Loaded {len(articles)} articles.\")\n",
    "\n",
    "# Function to get articles for a specific split\n",
    "def get_articles_for_split(split_data, articles):\n",
    "    split_articles = []\n",
    "    for article_id, bias in split_data:\n",
    "        if article_id in articles:\n",
    "            article = articles[article_id]\n",
    "            article[\"bias\"] = int(bias)  # Add bias label to the article\n",
    "            split_articles.append(article)\n",
    "    return split_articles\n",
    "\n",
    "# ----> Load the random_train data first <----\n",
    "splits_dir = \"./data/splits/\"  # Define the splits directory\n",
    "\n",
    "# Function to load a split file (if not already defined)\n",
    "def load_split(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return []\n",
    "    with open(file_path, \"r\", encoding = \"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        split_data = [line.strip().split(\"\\t\") for line in lines]  # Split by tab\n",
    "        return split_data\n",
    "\n",
    "random_train = load_split(os.path.join(splits_dir, \"random/train.tsv\"))\n",
    "random_test = load_split(os.path.join(splits_dir, \"random/test.tsv\"))\n",
    "# ----> Now you can use random_train <----\n",
    "\n",
    "# Get articles for the random train split\n",
    "random_train_articles = get_articles_for_split(random_train[1:], articles)  # Skip header\n",
    "\n",
    "# Get articles for the random test split  # This line is added\n",
    "random_test_articles = get_articles_for_split(random_test[1:], articles)  # Skip header and use random_test\n",
    "\n",
    "# Example: Print the first article in the random train split\n",
    "print(random_train_articles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tGP4ouvlF3Xb",
    "outputId": "3ca4c350-d092-4159-a3d4-4860ab46e8f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Article-Bias-Prediction/data/splits/random_train.tsv\n"
     ]
    }
   ],
   "source": [
    "splits_dir = \"./data/splits/\"\n",
    "file_path = os.path.join(splits_dir, \"random_train.tsv\")\n",
    "absolute_file_path = os.path.abspath(file_path)\n",
    "print(absolute_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ObCFiDCtDjve",
    "outputId": "d506ddfd-1ed7-4855-8855-8413dd276f42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of the repository:\n",
      "['LICENSE', '.gitignore', 'data', 'README.md', '.git']\n",
      "\n",
      "Contents of the data directory:\n",
      "['splits', 'jsons']\n",
      "\n",
      "Contents of the splits directory:\n",
      "['random', 'media']\n"
     ]
    }
   ],
   "source": [
    "# Check the contents of the repository\n",
    "repo_dir = \"/content/Article-Bias-Prediction\"\n",
    "print(\"Contents of the repository:\")\n",
    "print(os.listdir(repo_dir))\n",
    "\n",
    "# Check the contents of the data directory\n",
    "data_dir = os.path.join(repo_dir, \"data\")\n",
    "if os.path.exists(data_dir):\n",
    "    print(\"\\nContents of the data directory:\")\n",
    "    print(os.listdir(data_dir))\n",
    "else:\n",
    "    print(f\"\\nData directory not found: {data_dir}\")\n",
    "\n",
    "# Check the contents of the splits directory\n",
    "splits_dir = os.path.join(data_dir, \"splits\")\n",
    "if os.path.exists(splits_dir):\n",
    "    print(\"\\nContents of the splits directory:\")\n",
    "    print(os.listdir(splits_dir))\n",
    "else:\n",
    "    print(f\"\\nSplits directory not found: {splits_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cHk7WLv8Io2B",
    "outputId": "52c05537-6987-451e-982c-16f61623b6b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: /content/Article-Bias-Prediction/data/splits/random/val.tsv\n",
      "File not found: /content/Article-Bias-Prediction/data/splits/media/val.tsv\n",
      "Random Train Split:\n",
      "[['ID', 'bias'], ['IBgtqEHUEcjyMriR', '0'], ['LC3zdsbACLILzBhY', '2'], ['W084H19Vniu1Z24f', '2'], ['Lcvv69hlu4J5Foc3', '1']]\n",
      "\n",
      "Media Train Split:\n",
      "[['ID', 'bias'], ['zl7kc7EmAyIdUMIo', '2'], ['xpbjYTJYPdlw6HmJ', '1'], ['k4SGI3GXarnz5dJl', '0'], ['0jIpietfnrPRGHKQ', '1']]\n"
     ]
    }
   ],
   "source": [
    "# Function to load a split file\n",
    "def load_split(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return []\n",
    "    with open(file_path, \"r\", encoding = \"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        split_data = [line.strip().split(\"\\t\") for line in lines]  # Split by tab\n",
    "        return split_data\n",
    "\n",
    "# Load train, validation, and test splits for random and media-based splits\n",
    "random_train = load_split(os.path.join(splits_dir, \"random/train.tsv\"))\n",
    "random_val = load_split(os.path.join(splits_dir, \"random/val.tsv\"))\n",
    "random_test = load_split(os.path.join(splits_dir, \"random/test.tsv\"))\n",
    "\n",
    "media_train = load_split(os.path.join(splits_dir, \"media/train.tsv\"))\n",
    "media_val = load_split(os.path.join(splits_dir, \"media/val.tsv\"))\n",
    "media_test = load_split(os.path.join(splits_dir, \"media/test.tsv\"))\n",
    "\n",
    "# Example: Print the first few entries of the random train split\n",
    "print(\"Random Train Split:\")\n",
    "print(random_train[:5])\n",
    "\n",
    "# Example: Print the first few entries of the media train split\n",
    "print(\"\\nMedia Train Split:\")\n",
    "print(media_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HZxnnK0UJEWb",
    "outputId": "88713a59-7ed6-43a7-ba4e-acdb35edcb4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in random directory:\n",
      "['test.tsv', 'train.tsv', 'valid.tsv']\n",
      "\n",
      "Files in media directory:\n",
      "['test.tsv', 'train.tsv', 'valid.tsv']\n"
     ]
    }
   ],
   "source": [
    "# List files in the random and media subdirectories\n",
    "random_dir = os.path.join(splits_dir, \"random\")\n",
    "media_dir = os.path.join(splits_dir, \"media\")\n",
    "\n",
    "print(\"Files in random directory:\")\n",
    "print(os.listdir(random_dir))\n",
    "\n",
    "print(\"\\nFiles in media directory:\")\n",
    "print(os.listdir(media_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rs4aL4lSJawi",
    "outputId": "c7aeaf02-9b66-4112-e356-b6549f7f686f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Train Split:\n",
      "[['ID', 'bias'], ['IBgtqEHUEcjyMriR', '0'], ['LC3zdsbACLILzBhY', '2'], ['W084H19Vniu1Z24f', '2'], ['Lcvv69hlu4J5Foc3', '1']]\n",
      "\n",
      "Random Validation Split:\n",
      "[['ID', 'bias'], ['FtW2YUMthlUhzB0M', '2'], ['Jtd2uATJt8ptsUrK', '0'], ['NDh0a4dtfZZbfR9N', '0'], ['Bw62njQRNFCIj6GI', '1']]\n",
      "\n",
      "Media Train Split:\n",
      "[['ID', 'bias'], ['zl7kc7EmAyIdUMIo', '2'], ['xpbjYTJYPdlw6HmJ', '1'], ['k4SGI3GXarnz5dJl', '0'], ['0jIpietfnrPRGHKQ', '1']]\n",
      "\n",
      "Media Validation Split:\n",
      "[['ID', 'bias'], ['z0VHaSkKcMPpMDW7', '2'], ['LPHhXa5JgQ111mrv', '0'], ['U3B1ZlivPsMNeYOs', '0'], ['gnAmPbGB2s9ZSmC1', '0']]\n"
     ]
    }
   ],
   "source": [
    "# Function to load a split file\n",
    "def load_split(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return []\n",
    "    with open(file_path, \"r\", encoding = \"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        split_data = [line.strip().split(\"\\t\") for line in lines]  # Split by tab\n",
    "        return split_data\n",
    "\n",
    "# Load train, validation, and test splits for random and media-based splits\n",
    "random_train = load_split(os.path.join(splits_dir, \"random/train.tsv\"))\n",
    "random_val = load_split(os.path.join(splits_dir, \"random/valid.tsv\"))  # Updated to 'valid.tsv'\n",
    "random_test = load_split(os.path.join(splits_dir, \"random/test.tsv\"))\n",
    "\n",
    "media_train = load_split(os.path.join(splits_dir, \"media/train.tsv\"))\n",
    "media_val = load_split(os.path.join(splits_dir, \"media/valid.tsv\"))  # Updated to 'valid.tsv'\n",
    "media_test = load_split(os.path.join(splits_dir, \"media/test.tsv\"))\n",
    "\n",
    "# Example: Print the first few entries of the random train split\n",
    "print(\"Random Train Split:\")\n",
    "print(random_train[:5])\n",
    "\n",
    "# Example: Print the first few entries of the random validation split\n",
    "print(\"\\nRandom Validation Split:\")\n",
    "print(random_val[:5])\n",
    "\n",
    "# Example: Print the first few entries of the media train split\n",
    "print(\"\\nMedia Train Split:\")\n",
    "print(media_train[:5])\n",
    "\n",
    "# Example: Print the first few entries of the media validation split\n",
    "print(\"\\nMedia Validation Split:\")\n",
    "print(media_val[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "poaCupaIJigF",
    "outputId": "2dce8ebf-9623-40cc-ccc0-923329d90403"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic': 'coronavirus', 'source': 'The Guardian', 'bias': 0, 'url': 'https://www.theguardian.com/world/2020/mar/26/us-coronavirus-stimulus-all-you-need-to-know', 'title': 'US coronavirus stimulus checks: are you eligible and how much will you get?', 'date': '2020-03-26', 'authors': 'Lauren Aratani', 'content': \"Most taxpayers will get a check from the $ 2tn economic package , the largest in US history\\nUS coronavirus stimulus checks : are you eligible and how much will you get ?\\nThe US has agreed on a $ 2tn stimulus package , the largest economic stimulus in US history , in response to the economic impacts of Covid-19 . While corporations will be the biggest recipients of the bailout , some of that money will be paid directly to Americans hit by the pandemic .\\nMost taxpayers will get a check in the mail , while those directly affected by the economic effects of Covid-19 are slated to receive robust government support .\\nHere ’ s what we know so far about how the new stimulus package will directly affect Americans in the coming weeks :\\nCongress will spend about $ 250bn for checks up to $ 1,200 per person that will go directly to taxpayers .\\nTo be eligible for the full amount , a person ’ s most recently filed tax return must show that they made $ 75,000 or under . For couples , who can receive a maximum of $ 2,400 , the cutoff is $ 150,000 .\\nIf a person makes more than $ 75,000 , the amount given goes down incrementally by $ 5 for every $ 100 increase in salary . So a person who makes $ 85,000 would get $ 700 while a person who makes $ 95,000 would get $ 200 .\\nIf a person makes above $ 99,000 , or a couple makes above $ 198,000 , no check will be given .\\nThere are some requirements and exceptions . Those getting a check must be living and working in the US and have a valid social security number . If a person is listed as a dependent on their parents ’ tax return – the case for many college students – they are excluded .\\nThe Tax Foundation , a DC-based thinktank , estimates that 93.6 % of Americans will be eligible for a check coming from the stimulus package .\\nTaxpayers will be given $ 500 per child 16 or under listed as a dependent on their latest tax return .\\nThat is still unclear . Experts say that Americans will probably not get the money until May , given how long it takes the Internal Revenue Service ( IRS ) to send out checks . The treasury secretary , Steven Mnuchin , said those whose bank account information is already with the IRS from previous tax returns can expect a payment “ within three weeks ” .\\nFor those who still need to give the IRS their bank account information , a web-based portal will be opening soon to allow people to provide the IRS with their information . The IRS says this is to make sure people get their checks immediately , instead of sending them through the mail .\\nThis is not the first time the government has sent checks to Americans . The federal government gave up to $ 300 in 2001 and $ 600 in 2008 to taxpayers who met a certain income bracket to similarly stimulate the economy .\\nWhat if a person did not file their 2018 or 2019 return ?\\nThe IRS recommends people file their 2018 or 2019 tax return as soon as possible to get the payment . A check will be sent to any qualified person so long as they file their return within 2020 .\\nA person may still be eligible even if they do not file the taxes . The IRS says people who typically do not file taxes – low-income taxpayers and some veterans – will need to file a “ simple tax return ” in order to get the payment . Social security beneficiaries will not need to fill out the tax return , even if they do not file their taxes , to get the payment .\\nYes , specifically the bill will increase unemployment insurance by $ 600 for 13 weeks – about four months – for every person , added to the existing unemployment compensation a person gets from their state ’ s program .\\nThe length and amount of compensation varies from state to state . A majority of states providing a maximum of 26 weeks of compensation , while average weekly compensation ranges from 20 % of a person ’ s wage to just over 50 % .\\nWhile unemployment insurance typically does not cover people who are self-employed – freelancers , contractors and gig workers – the bill comes with a “ pandemic unemployment assistance ” measure that will extend insurance to those workers .\\nPlay Video 2:25 'The universe is collapsing ' : Bernie Sanders mocks Republicans over coronavirus aid – video\\nNot in this bill . Earlier last week , Donald Trump signed the Families First Coronavirus Response Act , a bill worth about $ 100bn meant to expand paid sick leave and emergency paid leave , but it came with major loopholes . Companies with over 500 employees were not mentioned in the bill , while companies with under 50 employees can apply for exemptions .\\nThe bill mandates 10 days of fully paid sick leave for employees of companies with 500 employees or less . Parents of those companies affected by their children ’ s school closing and those leaving for medical reasons can get 12 weeks of pay at 67 % of their salary .\\nWill there be anything to offset healthcare costs related to Covid-19 ?\\nAgain , not in this bill . The Families First Coronavirus Response Act included a measure that mandated all Covid-19 testing is free , but treatment for any symptoms ( there is currently no cure for the illness ) still comes at a cost .\\nA few states have reopened enrollment for their health insurance programs to allow those concerned about costs to enroll , and three major health insurance companies said they will be waiving any high costs for treatment , but there are still stories of people getting bills for as much as $ 34,000 to cover treatment of the virus ’ s symptoms .\", 'content_original': \"Most taxpayers will get a check from the $2tn economic package, the largest in US history\\n\\nUS coronavirus stimulus checks: are you eligible and how much will you get?\\n\\nThe US has agreed on a $2tn stimulus package, the largest economic stimulus in US history, in response to the economic impacts of Covid-19. While corporations will be the biggest recipients of the bailout, some of that money will be paid directly to Americans hit by the pandemic.\\n\\nMost taxpayers will get a check in the mail, while those directly affected by the economic effects of Covid-19 are slated to receive robust government support.\\n\\nHere’s what we know so far about how the new stimulus package will directly affect Americans in the coming weeks:\\n\\nWho’s eligible for a check from the government?\\n\\nCongress will spend about $250bn for checks up to $1,200 per person that will go directly to taxpayers.\\n\\nTo be eligible for the full amount, a person’s most recently filed tax return must show that they made $75,000 or under. For couples, who can receive a maximum of $2,400, the cutoff is $150,000.\\n\\nIf a person makes more than $75,000, the amount given goes down incrementally by $5 for every $100 increase in salary. So a person who makes $85,000 would get $700 while a person who makes $95,000 would get $200.\\n\\nIf a person makes above $99,000, or a couple makes above $198,000, no check will be given.\\n\\nThere are some requirements and exceptions. Those getting a check must be living and working in the US and have a valid social security number. If a person is listed as a dependent on their parents’ tax return – the case for many college students – they are excluded.\\n\\nThe Tax Foundation, a DC-based thinktank, estimates that 93.6% of Americans will be eligible for a check coming from the stimulus package.\\n\\nWhat about parents?\\n\\nTaxpayers will be given $500 per child 16 or under listed as a dependent on their latest tax return.\\n\\nWhen will I get this money?\\n\\nThat is still unclear. Experts say that Americans will probably not get the money until May, given how long it takes the Internal Revenue Service (IRS) to send out checks. The treasury secretary, Steven Mnuchin, said those whose bank account information is already with the IRS from previous tax returns can expect a payment “within three weeks”.\\n\\nFor those who still need to give the IRS their bank account information, a web-based portal will be opening soon to allow people to provide the IRS with their information. The IRS says this is to make sure people get their checks immediately, instead of sending them through the mail.\\n\\nThis is not the first time the government has sent checks to Americans. The federal government gave up to $300 in 2001 and $600 in 2008 to taxpayers who met a certain income bracket to similarly stimulate the economy.\\n\\nWhat if a person did not file their 2018 or 2019 return?\\n\\nThe IRS recommends people file their 2018 or 2019 tax return as soon as possible to get the payment. A check will be sent to any qualified person so long as they file their return within 2020.\\n\\nWhat if a person does not file their taxes?\\n\\nA person may still be eligible even if they do not file the taxes. The IRS says people who typically do not file taxes – low-income taxpayers and some veterans – will need to file a “simple tax return” in order to get the payment. Social security beneficiaries will not need to fill out the tax return, even if they do not file their taxes, to get the payment.\\n\\nDoes the package help unemployed workers?\\n\\nYes, specifically the bill will increase unemployment insurance by $600 for 13 weeks – about four months – for every person, added to the existing unemployment compensation a person gets from their state’s program.\\n\\nThe length and amount of compensation varies from state to state. A majority of states providing a maximum of 26 weeks of compensation, while average weekly compensation ranges from 20% of a person’s wage to just over 50%.\\n\\nWhat about freelance and gig workers?\\n\\nWhile unemployment insurance typically does not cover people who are self-employed – freelancers, contractors and gig workers – the bill comes with a “pandemic unemployment assistance” measure that will extend insurance to those workers.\\n\\nPlay Video 2:25 'The universe is collapsing': Bernie Sanders mocks Republicans over coronavirus aid – video\\n\\nHave there been any changes to paid leave?\\n\\nNot in this bill. Earlier last week, Donald Trump signed the Families First Coronavirus Response Act, a bill worth about $100bn meant to expand paid sick leave and emergency paid leave, but it came with major loopholes. Companies with over 500 employees were not mentioned in the bill, while companies with under 50 employees can apply for exemptions.\\n\\nThe bill mandates 10 days of fully paid sick leave for employees of companies with 500 employees or less. Parents of those companies affected by their children’s school closing and those leaving for medical reasons can get 12 weeks of pay at 67% of their salary.\\n\\nWill there be anything to offset healthcare costs related to Covid-19?\\n\\nAgain, not in this bill. The Families First Coronavirus Response Act included a measure that mandated all Covid-19 testing is free, but treatment for any symptoms (there is currently no cure for the illness) still comes at a cost.\\n\\nA few states have reopened enrollment for their health insurance programs to allow those concerned about costs to enroll, and three major health insurance companies said they will be waiving any high costs for treatment, but there are still stories of people getting bills for as much as $34,000 to cover treatment of the virus’s symptoms.\", 'source_url': 'www.theguardian.com', 'bias_text': 'left', 'ID': 'IBgtqEHUEcjyMriR'}\n"
     ]
    }
   ],
   "source": [
    "# Path to the jsons directory\n",
    "jsons_dir = \"./data/jsons/\"\n",
    "\n",
    "# Load all JSON articles into a dictionary\n",
    "articles = {}\n",
    "for filename in os.listdir(jsons_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(jsons_dir, filename)\n",
    "        with open(file_path, \"r\", encoding = \"utf-8\") as f:\n",
    "            article = json.load(f)\n",
    "            articles[article[\"ID\"]] = article\n",
    "\n",
    "# Function to get articles for a specific split\n",
    "def get_articles_for_split(split_data, articles):\n",
    "    split_articles = []\n",
    "    for article_id, bias in split_data:\n",
    "        if article_id in articles:\n",
    "            article = articles[article_id]\n",
    "            article[\"bias\"] = int(bias)  # Add bias label to the article\n",
    "            split_articles.append(article)\n",
    "    return split_articles\n",
    "\n",
    "# Get articles for the random train split\n",
    "random_train_articles = get_articles_for_split(random_train[1:], articles)  # Skip header\n",
    "\n",
    "# Example: Print the first article in the random train split\n",
    "print(random_train_articles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Ubjt1Q7obG4",
    "outputId": "51ffb07d-9a7a-4907-c133-d203c3ab03b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DataFrame shape: 27978\n",
      "Validation DataFrame shape: 6996\n",
      "Test DataFrame shape: 1300\n"
     ]
    }
   ],
   "source": [
    "# Get articles for the random train split\n",
    "random_train_articles = get_articles_for_split(random_train[1:], articles)  # Skip header, use random_train\n",
    "\n",
    "# Get articles for the random validation split\n",
    "random_val_articles = get_articles_for_split(random_val[1:], articles)  # Skip header, use random_val\n",
    "\n",
    "# Get articles for the random test split\n",
    "random_test_articles = get_articles_for_split(random_test[1:], articles)  # Skip header, use random_test\n",
    "\n",
    "# Now you can create DataFrames if needed using the lists of articles\n",
    "# Example:\n",
    "# train_pd_df = pd.DataFrame(random_train_articles)\n",
    "# valid_pd_df = pd.DataFrame(random_val_articles)\n",
    "# test_pd_df = pd.DataFrame(random_test_articles)\n",
    "\n",
    "print(\"Training DataFrame shape:\", len(random_train_articles)) # Adjust to len for list length\n",
    "print(\"Validation DataFrame shape:\", len(random_val_articles))\n",
    "print(\"Test DataFrame shape:\", len(random_test_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TDMRaiXNpPR7",
    "outputId": "c858f5ee-e3db-4ac7-9f1f-479b33b6f98c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Julian Zelizer', 'Rev. Jesse Jackson Sr.', 'American Enterprise Institute', 'CBS News', 'New York Post', 'Ted Rall (cartoonist)', 'Ben Shapiro', 'Newsmax - News', 'Dick Morris', 'TheBlaze.com', 'Carol Costello', 'The Atlantic', 'The Epoch Times', 'Noah Rothman', 'Kathleen Parker', 'New York Post (Opinion)', 'Mother Jones', 'Ann Coulter', 'ThinkProgress', 'Washington Times', 'Reuters', 'CNN - Editorial', 'Ross Douthat', 'Ben Stein', 'Jonathan Chait', 'Piers Morgan', 'Associated Press', 'RealClearPolitics', 'Yahoo! News', 'Howard Kurtz', 'NPR Editorial ', 'Newsmax - Opinion', 'NPR Online News', 'Jacobin', 'NBCNews.com', 'Mitt Romney', 'Dana Milbank', 'New York Times (Online News)', 'John Stossel', 'Politico', 'Vice', 'The Intelligencer', 'Fox News', 'Reason', 'Michael Goodwin', 'AP Fact Check', 'George Will', 'Business Insider', 'The Hill', 'The New Yorker', 'HotAir', 'Andrew Napolitano', 'Guest Writer - Center', 'Townhall', 'Media Research Center', 'Pat Buchanan', 'Matt Welch', 'Daily Mail', 'Damon Linker', 'BuzzFeed News', 'Rand Paul', 'Bret Stephens', 'Washington Post', 'S.E. Cupp', 'Fox News Opinion', 'NBC News (Online)', 'Michael Kinsley', 'Allysia Finley (Wall Street Journal)', 'Michael Brendan Dougherty', 'Tucker Carlson', 'Ezra Klein', 'International Business Times', 'Jonathan Haidt', 'MarketWatch', 'The Boston Globe', 'Guest Writer', 'International Institute for Strategic Studies', 'Annafi Wahed', 'CBN', 'Yahoo! The 360', 'Charles Blow', 'Billy Binion', 'Elizabeth Warren', 'National Review', 'Mitch McConnell', 'Al Jazeera', 'Nicholas Kristof', 'Tom Nichols', 'Matt Towery', 'Polish Times', 'Walt Handelsman (cartoonist)', 'John Boehner', 'Dennis Prager', 'William Bennett', 'The Week - News', 'CNN (Web News)', 'Newt Gingrich', 'AllSides', 'American Spectator', 'ABC News', 'The Nation', 'Scientific American', 'New York Magazine', 'Michael Barone', 'The Economist', 'Wall Street Journal - Editorial', 'Jonah Goldberg', 'Salon', 'Charles Krauthammer', 'Guest Writer - Right', 'NewsBusters', 'Washington Free Beacon', 'Jim Obergefell', 'Lisa Gable', 'Christopher Buskirk', 'Scott Walker', 'Peter Thiel', 'New York Times Editorial Board', 'The Marshall Project', 'David Leonhardt', 'The Daily Wire', 'Jon Terbush', 'Newsmax (News)', 'Media Matters', 'Victor Hanson', 'Time Magazine', 'Pew Research Center', 'USA TODAY', 'Thomas Frank', 'The Week - Opinion', 'BBC News', 'Frank Bruni', 'Michelle Malkin', 'The Guardian', 'Rem Reider', 'Guest Writer - Left', 'Andrew Sullivan', 'Thomas Sowell', 'NBC (Web News)', 'Washington Examiner', 'Newsmax', 'The Intercept', 'Chicago Sun-Times', 'Breitbart News', 'Rich Lowry', 'John Fund', 'The Flip Side', 'Joan Blades', 'Vox', 'Jeff Jacoby', 'Aaron Carroll', 'The Daily Caller', 'Christian Science Monitor', 'New York Post (News)', 'Jesse Jackson', 'Daily Kos', 'Ryan Cooper', 'Carrie Lukas', 'Daily Beast', 'FiveThirtyEight', 'Scott Jennings', 'Michelle Goldberg', 'Bloomberg', 'Fox News (Online)', 'Barack Obama', 'Amy Klobuchar', 'Conor Friedersdorf', 'Chris Ruddy', 'New York Times - Opinion', 'David Brooks', 'The American Spectator', 'Jonathan Miller', 'Yellow Scene Magazine', 'Brent Bozell', 'Richard A. Lowry', 'Tom Cole', 'Ken Burns', 'Fox Online News', 'Democracy Now', 'CNBC', 'Juan Williams', 'Gail Collins', 'Axios', 'Ralph Benko', 'New York Times - News', 'Slate', 'James Bovard', 'Wall Street Journal - News', 'ProPublica', 'ABC News (Online)', 'HuffPost', 'Vanity Fair']\n"
     ]
    }
   ],
   "source": [
    "# Print all media sources from the training dataset\n",
    "sources = [article['source'] for article in random_train_articles]\n",
    "unique_sources = list(set(sources)) # Get unique sources\n",
    "print(unique_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fZsPVrHhBALN",
    "outputId": "67674281-f977-45f2-f67d-94f2dda53973"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Julian Zelizer', 'Rev. Jesse Jackson Sr.', 'American Enterprise Institute', 'CBS News', 'New York Post', 'Ted Rall (cartoonist)', 'Ben Shapiro', 'Newsmax - News', 'Dick Morris', 'TheBlaze.com', 'The Atlantic', 'The Epoch Times', 'Noah Rothman', 'Kathleen Parker', 'New York Post (Opinion)', 'Mother Jones', 'Ann Coulter', 'ThinkProgress', 'Washington Times', 'Reuters', 'CNN - Editorial', 'Ross Douthat', 'Ben Stein', 'Jonathan Chait', 'Piers Morgan', 'Associated Press', 'RealClearPolitics', 'Yahoo! News', 'Howard Kurtz', 'NPR Editorial ', 'Newsmax - Opinion', 'NPR Online News', 'Jacobin', 'NBCNews.com', 'Mitt Romney', 'Dana Milbank', 'New York Times (Online News)', 'John Stossel', 'Politico', 'Vice', 'Fox News', 'Reason', 'Michael Goodwin', 'AP Fact Check', 'George Will', 'Business Insider', 'The Hill', 'The New Yorker', 'HotAir', 'Andrew Napolitano', 'Guest Writer - Center', 'Townhall', 'Media Research Center', 'Pat Buchanan', 'Matt Welch', 'Daily Mail', 'Damon Linker', 'BuzzFeed News', 'Rand Paul', 'Bret Stephens', 'Washington Post', 'S.E. Cupp', 'Fox News Opinion', 'NBC News (Online)', 'Michael Kinsley', 'Allysia Finley (Wall Street Journal)', 'Michael Brendan Dougherty', 'Tucker Carlson', 'Ezra Klein', 'International Business Times', 'Jonathan Haidt', 'MarketWatch', 'The Boston Globe', 'International Institute for Strategic Studies', 'Annafi Wahed', 'CBN', 'Yahoo! The 360', 'Charles Blow', 'Billy Binion', 'Elizabeth Warren', 'National Review', 'Mitch McConnell', 'Al Jazeera', 'Nicholas Kristof', 'Tom Nichols', 'Matt Towery', 'Walt Handelsman (cartoonist)', 'John Boehner', 'Dennis Prager', 'William Bennett', 'The Week - News', 'CNN (Web News)', 'Newt Gingrich', 'AllSides', 'American Spectator', 'ABC News', 'The Nation', 'Scientific American', 'New York Magazine', 'Michael Barone', 'The Economist', 'Wall Street Journal - Editorial', 'Jonah Goldberg', 'Salon', 'Charles Krauthammer', 'Guest Writer - Right', 'NewsBusters', 'Washington Free Beacon', 'Jim Obergefell', 'Lisa Gable', 'Christopher Buskirk', 'Scott Walker', 'Peter Thiel', 'New York Times Editorial Board', 'The Marshall Project', 'David Leonhardt', 'The Daily Wire', 'Jon Terbush', 'Newsmax (News)', 'Media Matters', 'Victor Hanson', 'Time Magazine', 'Pew Research Center', 'USA TODAY', 'Thomas Frank', 'The Week - Opinion', 'BBC News', 'Frank Bruni', 'Michelle Malkin', 'The Guardian', 'Rem Reider', 'Guest Writer - Left', 'Andrew Sullivan', 'Thomas Sowell', 'NBC (Web News)', 'Washington Examiner', 'Newsmax', 'The Intercept', 'Chicago Sun-Times', 'Breitbart News', 'Rich Lowry', 'John Fund', 'The Flip Side', 'Joan Blades', 'Vox', 'Jeff Jacoby', 'The Daily Caller', 'Christian Science Monitor', 'New York Post (News)', 'Jesse Jackson', 'Daily Kos', 'Ryan Cooper', 'Carrie Lukas', 'Daily Beast', 'FiveThirtyEight', 'Scott Jennings', 'Michelle Goldberg', 'Bloomberg', 'Fox News (Online)', 'Barack Obama', 'Amy Klobuchar', 'Conor Friedersdorf', 'Chris Ruddy', 'New York Times - Opinion', 'David Brooks', 'The American Spectator', 'Jonathan Miller', 'Brent Bozell', 'Richard A. Lowry', 'Tom Cole', 'Fox Online News', 'Democracy Now', 'CNBC', 'Juan Williams', 'Gail Collins', 'Axios', 'Ralph Benko', 'New York Times - News', 'Slate', 'James Bovard', 'Wall Street Journal - News', 'ProPublica', 'ABC News (Online)', 'HuffPost', 'Vanity Fair']\n"
     ]
    }
   ],
   "source": [
    "# Define sources to drop\n",
    "drop_list = {\"Guest Writer\", \"Yellow Scene Magazine\", \"Aaron Carroll\", \"Ken Burns\",\n",
    "             \"Carol Costello\", \"The Intelligencer\", \"Polish Times\"}\n",
    "\n",
    "# Drop the specified sources\n",
    "unique_sources = [source for source in unique_sources if source not in drop_list]\n",
    "\n",
    "print(unique_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uTvw5ySDysDb",
    "outputId": "1f146303-4c5b-4bb4-e513-44ab5190e189"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ABC News': np.int64(0), 'ABC News (Online)': np.int64(1), 'AP Fact Check': np.int64(2), 'Al Jazeera': np.int64(3), 'AllSides': np.int64(4), 'Allysia Finley (Wall Street Journal)': np.int64(5), 'American Enterprise Institute': np.int64(6), 'American Spectator': np.int64(7), 'Amy Klobuchar': np.int64(8), 'Andrew Napolitano': np.int64(9), 'Andrew Sullivan': np.int64(10), 'Ann Coulter': np.int64(11), 'Annafi Wahed': np.int64(12), 'Associated Press': np.int64(13), 'Axios': np.int64(14), 'BBC News': np.int64(15), 'Barack Obama': np.int64(16), 'Ben Shapiro': np.int64(17), 'Ben Stein': np.int64(18), 'Billy Binion': np.int64(19), 'Bloomberg': np.int64(20), 'Breitbart News': np.int64(21), 'Brent Bozell': np.int64(22), 'Bret Stephens': np.int64(23), 'Business Insider': np.int64(24), 'BuzzFeed News': np.int64(25), 'CBN': np.int64(26), 'CBS News': np.int64(27), 'CNBC': np.int64(28), 'CNN (Web News)': np.int64(29), 'CNN - Editorial': np.int64(30), 'Carrie Lukas': np.int64(31), 'Charles Blow': np.int64(32), 'Charles Krauthammer': np.int64(33), 'Chicago Sun-Times': np.int64(34), 'Chris Ruddy': np.int64(35), 'Christian Science Monitor': np.int64(36), 'Christopher Buskirk': np.int64(37), 'Conor Friedersdorf': np.int64(38), 'Daily Beast': np.int64(39), 'Daily Kos': np.int64(40), 'Daily Mail': np.int64(41), 'Damon Linker': np.int64(42), 'Dana Milbank': np.int64(43), 'David Brooks': np.int64(44), 'David Leonhardt': np.int64(45), 'Democracy Now': np.int64(46), 'Dennis Prager': np.int64(47), 'Dick Morris': np.int64(48), 'Elizabeth Warren': np.int64(49), 'Ezra Klein': np.int64(50), 'FiveThirtyEight': np.int64(51), 'Fox News': np.int64(52), 'Fox News (Online)': np.int64(53), 'Fox News Opinion': np.int64(54), 'Fox Online News': np.int64(55), 'Frank Bruni': np.int64(56), 'Gail Collins': np.int64(57), 'George Will': np.int64(58), 'Guest Writer - Center': np.int64(59), 'Guest Writer - Left': np.int64(60), 'Guest Writer - Right': np.int64(61), 'HotAir': np.int64(62), 'Howard Kurtz': np.int64(63), 'HuffPost': np.int64(64), 'International Business Times': np.int64(65), 'International Institute for Strategic Studies': np.int64(66), 'Jacobin': np.int64(67), 'James Bovard': np.int64(68), 'Jeff Jacoby': np.int64(69), 'Jesse Jackson': np.int64(70), 'Jim Obergefell': np.int64(71), 'Joan Blades': np.int64(72), 'John Boehner': np.int64(73), 'John Fund': np.int64(74), 'John Stossel': np.int64(75), 'Jon Terbush': np.int64(76), 'Jonah Goldberg': np.int64(77), 'Jonathan Chait': np.int64(78), 'Jonathan Haidt': np.int64(79), 'Jonathan Miller': np.int64(80), 'Juan Williams': np.int64(81), 'Julian Zelizer': np.int64(82), 'Kathleen Parker': np.int64(83), 'Lisa Gable': np.int64(84), 'MarketWatch': np.int64(85), 'Matt Towery': np.int64(86), 'Matt Welch': np.int64(87), 'Media Matters': np.int64(88), 'Media Research Center': np.int64(89), 'Michael Barone': np.int64(90), 'Michael Brendan Dougherty': np.int64(91), 'Michael Goodwin': np.int64(92), 'Michael Kinsley': np.int64(93), 'Michelle Goldberg': np.int64(94), 'Michelle Malkin': np.int64(95), 'Mitch McConnell': np.int64(96), 'Mitt Romney': np.int64(97), 'Mother Jones': np.int64(98), 'NBC (Web News)': np.int64(99), 'NBC News (Online)': np.int64(100), 'NBCNews.com': np.int64(101), 'NPR Editorial ': np.int64(102), 'NPR Online News': np.int64(103), 'National Review': np.int64(104), 'New York Magazine': np.int64(105), 'New York Post': np.int64(106), 'New York Post (News)': np.int64(107), 'New York Post (Opinion)': np.int64(108), 'New York Times (Online News)': np.int64(109), 'New York Times - News': np.int64(110), 'New York Times - Opinion': np.int64(111), 'New York Times Editorial Board': np.int64(112), 'NewsBusters': np.int64(113), 'Newsmax': np.int64(114), 'Newsmax (News)': np.int64(115), 'Newsmax - News': np.int64(116), 'Newsmax - Opinion': np.int64(117), 'Newt Gingrich': np.int64(118), 'Nicholas Kristof': np.int64(119), 'Noah Rothman': np.int64(120), 'Pat Buchanan': np.int64(121), 'Peter Thiel': np.int64(122), 'Pew Research Center': np.int64(123), 'Piers Morgan': np.int64(124), 'Politico': np.int64(125), 'ProPublica': np.int64(126), 'Ralph Benko': np.int64(127), 'Rand Paul': np.int64(128), 'RealClearPolitics': np.int64(129), 'Reason': np.int64(130), 'Rem Reider': np.int64(131), 'Reuters': np.int64(132), 'Rev. Jesse Jackson Sr.': np.int64(133), 'Rich Lowry': np.int64(134), 'Richard A. Lowry': np.int64(135), 'Ross Douthat': np.int64(136), 'Ryan Cooper': np.int64(137), 'S.E. Cupp': np.int64(138), 'Salon': np.int64(139), 'Scientific American': np.int64(140), 'Scott Jennings': np.int64(141), 'Scott Walker': np.int64(142), 'Slate': np.int64(143), 'Ted Rall (cartoonist)': np.int64(144), 'The American Spectator': np.int64(145), 'The Atlantic': np.int64(146), 'The Boston Globe': np.int64(147), 'The Daily Caller': np.int64(148), 'The Daily Wire': np.int64(149), 'The Economist': np.int64(150), 'The Epoch Times': np.int64(151), 'The Flip Side': np.int64(152), 'The Guardian': np.int64(153), 'The Hill': np.int64(154), 'The Intercept': np.int64(155), 'The Marshall Project': np.int64(156), 'The Nation': np.int64(157), 'The New Yorker': np.int64(158), 'The Week - News': np.int64(159), 'The Week - Opinion': np.int64(160), 'TheBlaze.com': np.int64(161), 'ThinkProgress': np.int64(162), 'Thomas Frank': np.int64(163), 'Thomas Sowell': np.int64(164), 'Time Magazine': np.int64(165), 'Tom Cole': np.int64(166), 'Tom Nichols': np.int64(167), 'Townhall': np.int64(168), 'Tucker Carlson': np.int64(169), 'USA TODAY': np.int64(170), 'Vanity Fair': np.int64(171), 'Vice': np.int64(172), 'Victor Hanson': np.int64(173), 'Vox': np.int64(174), 'Wall Street Journal - Editorial': np.int64(175), 'Wall Street Journal - News': np.int64(176), 'Walt Handelsman (cartoonist)': np.int64(177), 'Washington Examiner': np.int64(178), 'Washington Free Beacon': np.int64(179), 'Washington Post': np.int64(180), 'Washington Times': np.int64(181), 'William Bennett': np.int64(182), 'Yahoo! News': np.int64(183), 'Yahoo! The 360': np.int64(184)}\n",
      "             source  source_encoded\n",
      "0      The Guardian             153\n",
      "1  Washington Times             181\n",
      "2   National Review             104\n",
      "3   NPR Online News             103\n",
      "4         Bloomberg              20\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with the articles\n",
    "df = pd.DataFrame(random_train_articles)  # Assuming random_train_articles is a list of dictionaries\n",
    "\n",
    "# Filter out the unwanted sources\n",
    "df = df[~df['source'].isin(drop_list)]\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the 'source' column\n",
    "df['source_encoded'] = label_encoder.fit_transform(df['source'])\n",
    "\n",
    "# Print the mapping of source names to integers\n",
    "source_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(source_mapping)\n",
    "\n",
    "# Show the first few rows\n",
    "print(df[['source', 'source_encoded']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EHzWBwFvjrga"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Ratings received from https://www.allsides.com/media-bias/ratings as of 21 March 2025\n",
    "# There are 5 categories: left (0), lean left (0.5), center (1), lean right (1.5), right (2)\n",
    "# Each of the category will receive\n",
    "allsides_media_bias_ratings = {\n",
    "    \"The Guardian\": 0,\n",
    "    \"Washington Times\": 1.5,\n",
    "    \"National Review\": 1.5,\n",
    "    \"NPR Online News\": 0.5,\n",
    "    \"Bloomberg\": 0.5,\n",
    "    \"Reuters\": 1,\n",
    "    \"Politico\": 0.5,\n",
    "    \"Christian Science Monitor\": 1,\n",
    "    \"TheBlaze.com\": 2,\n",
    "    \"Fox Online News\": 2,\n",
    "    \"ABC News\": 0.5,\n",
    "    \"USA TODAY\": 0.5,\n",
    "    \"CNN (Web News)\": 0.5,\n",
    "    \"Guest Writer - Left\": 0.5,\n",
    "    \"The Hill\": 1,\n",
    "    \"New York Times - News\": 0.5,\n",
    "    \"Associated Press\": 0,\n",
    "    \"Daily Beast\": 0,\n",
    "    \"Townhall\": 2,\n",
    "    \"Guest Writer - Right\": 1.5,\n",
    "    \"Salon\": 0,\n",
    "    \"CBS News\": 0.5,\n",
    "    \"New York Post\": 1.5,\n",
    "    \"CBN\": 2,\n",
    "    \"Media Matters\": 0,\n",
    "    \"Rich Lowry\": 2,\n",
    "    \"Breitbart News\": 2,\n",
    "    \"Newsmax - News\": 2,\n",
    "    \"New York Times - Opinion\": 0,\n",
    "    \"BBC News\": 1,\n",
    "    \"ThinkProgress\": 0,\n",
    "    \"RealClearPolitics\": 1.5,\n",
    "    \"Newsmax\": 2,\n",
    "    \"The Atlantic\": 0,\n",
    "    \"International Business Times\": 1,\n",
    "    \"The Flip Side\": 1,\n",
    "    \"Scientific American\": 0.5,\n",
    "    \"CNN - Editorial\": 0,\n",
    "    \"Democracy Now\": 0,\n",
    "    \"The Week - News\": 0.5,\n",
    "    \"Fox News (Online)\": 2,\n",
    "    \"Ezra Klein\": 0,\n",
    "    \"The Daily Caller\": 2,\n",
    "    \"Yahoo! The 360\": 1,\n",
    "    \"HotAir\": 1.5,\n",
    "    \"MarketWatch\": 1,\n",
    "    \"The Intercept\": 0,\n",
    "    \"Axios\": 0.5,\n",
    "    \"Reason\": 1,\n",
    "    \"John Stossel\": 1.5,\n",
    "    \"Daily Kos\": 0,\n",
    "    \"Daily Mail\": 2,\n",
    "    \"ProPublica\": 0.5,\n",
    "    \"CNBC\": 1,\n",
    "    \"Wall Street Journal - News\": 1,\n",
    "    \"Howard Kurtz\": 1,\n",
    "    \"Al Jazeera\": 0.5,\n",
    "    \"Vanity Fair\": 0.5,\n",
    "    \"Jacobin\": 0,\n",
    "    \"The Economist\": 0.5,\n",
    "    \"Dana Milbank\": 0,\n",
    "    \"Slate\": 0,\n",
    "    \"NBC News (Online)\": 0.5,\n",
    "    \"Vice\": 0,\n",
    "    \"The Nation\": 0,\n",
    "    \"The Epoch Times\": 1.5,\n",
    "    \"Victor Hanson\": 1.5,\n",
    "    \"Charles Krauthammer\": 1.5,\n",
    "    \"Rev. Jesse Jackson Sr.\": 0,\n",
    "    \"Ben Shapiro\": 2,\n",
    "    \"NewsBusters\": 2,\n",
    "    \"New York Magazine\": 0,\n",
    "    \"Time Magazine\": 0.5,\n",
    "    \"Peter Thiel\": 1.5,\n",
    "    \"Newt Gingrich\": 2,\n",
    "    \"Mother Jones\": 0,\n",
    "    \"Fox News Opinion\": 2,\n",
    "    \"Michael Barone\": 1.5,\n",
    "    \"Washington Free Beacon\": 2,\n",
    "    \"The Daily Wire\": 2,\n",
    "    \"New York Times (Online News)\": 0.5,\n",
    "    \"The Boston Globe\": 0,\n",
    "    \"Chicago Sun-Times\": 0.5,\n",
    "    \"Michelle Malkin\": 2,\n",
    "    \"Andrew Napolitano\": 2,\n",
    "    \"Pew Research Center\": 1,\n",
    "    \"Media Research Center\": 2,\n",
    "    \"New York Post (Opinion)\": 2,\n",
    "    \"NBCNews.com\": 0.5,\n",
    "    \"BuzzFeed News\": 0,\n",
    "    \"Yahoo! News\": 0.5,\n",
    "    \"NPR Editorial \": 0.5,\n",
    "    \"The Marshall Project\": 1,\n",
    "    \"Scott Walker\": 2,\n",
    "    \"The New Yorker\": 0,\n",
    "    \"New York Post (News)\": 1.5,\n",
    "    \"Newsmax - Opinion\": 2,\n",
    "    \"The Week - Opinion\": 0.5,\n",
    "    \"Jonah Goldberg\": 2,\n",
    "    \"Richard A. Lowry\": 2,\n",
    "    \"John Fund\": 1.5,\n",
    "    \"AllSides\": 1, # Fixed: Added a colon\n",
    "    \"Newsmax (News)\": 2,\n",
    "    \"Christopher Buskirk\": 1.5, # Fixed: Added a comma\n",
    "    \"AP Fact Check\": 0.5,\n",
    "    \"American Enterprise Institute\": 1.5, # Fixed: Removed extra comma\n",
    "    \"Washington Examiner\": 1.5,\n",
    "    \"Noah Rothman\": 1.5,\n",
    "    \"Ann Coulter\": 2,\n",
    "    \"Kathleen Parker\": 1.5,\n",
    "    \"Mitch McConnell\": 1.5,\n",
    "    \"Barack Obama\": 0.5,\n",
    "    \"Dennis Prager\": 2,\n",
    "    \"Thomas Sowell\": 1.5,\n",
    "    \"Andrew Sullivan\": 0,\n",
    "    \"Washington Post\": 0.5,\n",
    "    \"Jesse Jackson\": 0,\n",
    "    \"William Bennett\": 2,\n",
    "    \"International Institute for Strategic Studies\": 1,\n",
    "    \"Jonathan Chait\": 0,\n",
    "    \"Matt Towery\": 1.5,\n",
    "    \"Guest Writer - Center\": 1,\n",
    "    \"Bret Stephens\": 1.5,\n",
    "    \"Michael Goodwin\": 2,\n",
    "    \"American Spectator\": 2,\n",
    "    \"The American Spectator\": 2,\n",
    "    \"Allysia Finley (Wall Street Journal)\": 2,\n",
    "    \"Pat Buchanan\": 2,\n",
    "    \"Ben Stein\": 1.5,\n",
    "    \"Michael Brendan Dougherty\": 1.5,\n",
    "    \"Mitt Romney\": 1.5,\n",
    "    \"Joan Blades\": 0,\n",
    "    \"Rem Reider\": 1,\n",
    "    \"Fox News\": 2,\n",
    "    \"Juan Williams\": 0.5,\n",
    "    \"David Brooks\": 1.5,\n",
    "    \"Ted Rall (cartoonist)\": 0.5,\n",
    "    \"Billy Binion\": 1,\n",
    "    \"Brent Bozell\": 2,\n",
    "    \"Charles Blow\": 0.5,\n",
    "    \"Conor Friedersdorf\": 1,\n",
    "    \"Damon Linker\": 0.5,\n",
    "    \"Elizabeth Warren\": 0,\n",
    "    \"Gail Collins\": 0,\n",
    "    \"Jim Obergefell\": 0.5,\n",
    "    \"Jonathan Haidt\": 1,\n",
    "    \"Julian Zelizer\": 0.5,\n",
    "    \"Lisa Gable\": 2,\n",
    "    \"Michael Kinsley\": 0,\n",
    "    \"Annafi Wahed\": 0.5,\n",
    "    \"David Leonhardt\": 0.5,\n",
    "    \"Ryan Cooper\": 0,\n",
    "    \"S.E. Cupp\": 1.5,\n",
    "    \"Scott Jennings\": 1.5,\n",
    "    \"Tom Nichols\": 1.5,\n",
    "    \"ABC News (Online)\": 0.5,\n",
    "    \"Business Insider\": 0.5,\n",
    "    \"HuffPost\": 0,\n",
    "    \"Ross Douthat\": 1.5,\n",
    "    \"James Bovard\": 1.5,\n",
    "    \"Tucker Carlson\": 2,\n",
    "    \"George Will\": 2,\n",
    "    \"Thomas Frank\": 0.5,\n",
    "    \"Jeff Jacoby\": 2,\n",
    "    \"Walt Handelsman (cartoonist)\": 0,\n",
    "    \"Piers Morgan\": 0.5,\n",
    "    \"Chris Ruddy\": 2,\n",
    "    \"Salon\": 0,\n",
    "    \"Michelle Goldberg\": 0,\n",
    "    \"Jonathan Miller\": 0,\n",
    "    \"Rand Paul\": 1.5,\n",
    "    \"Nicholas Kristof\": 0,\n",
    "    \"FiveThirtyEight\": 1,\n",
    "    \"Jon Terbush\": 0.5,\n",
    "    \"Matt Welch\": 1.5,\n",
    "    \"Vox\": 0,\n",
    "    \"Amy Klobuchar\": 0.5,\n",
    "    \"Dick Morris\": 1.5,\n",
    "    \"Carrie Lukas\": 2,\n",
    "    \"Ralph Benko\": 2,\n",
    "    \"Frank Bruni\": 0.5,\n",
    "    \"John Boehner\": 2,\n",
    "    \"Tom Cole\": 2,\n",
    "    \"New York Times Editorial Board\": 0,\n",
    "    \"NBC (Web News)\": 0.5,\n",
    "    \"Wall Street Journal - Editorial\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5MGw0EaNA6F"
   },
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yRSvE1ChKOY7"
   },
   "source": [
    "**Analyze the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5fsqvGBGKGb0",
    "outputId": "726f3008-5517-4582-df6f-14d6c4c12165"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({2: 10240, 0: 9750, 1: 7988})\n"
     ]
    }
   ],
   "source": [
    "# Count distributions of bias labels\n",
    "from collections import Counter\n",
    "\n",
    "bias_counts = Counter(article[\"bias\"] for article in random_train_articles)\n",
    "print(bias_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5yxeqZoMKQKc",
    "outputId": "0e9b06b6-0f02-416e-fc61-968d41c55751"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'elections': 4519, 'politics': 1692, 'white_house': 1474, 'immigration': 1237, 'healthcare': 1082, 'media_bias': 826, 'coronavirus': 800, 'middle_east': 695, 'gun_control_and_gun_rights': 644, 'supreme_court': 605, 'us_senate': 549, 'economy_and_jobs': 517, 'us_house': 498, 'violence_in_america': 495, 'federal_budget': 481, 'environment': 438, 'world': 432, 'republican_party': 419, 'national_security': 414, 'foreign_policy': 401, 'us_congress': 393, 'terrorism': 369, 'race_and_racism': 331, 'polarization': 330, 'education': 315, 'impeachment': 309, 'trade': 291, 'taxes': 291, 'abortion': 263, 'north_korea': 238, 'fbi': 238, 'culture': 232, 'justice': 230, 'general_news': 224, 'lgbt_rights': 221, 'justice_department': 212, 'free_speech': 199, 'us_military': 186, 'nsa': 183, 'technology': 179, 'democratic_party': 167, 'defense': 165, 'banking_and_finance': 161, 'religion_and_faith': 159, 'economic_policy': 156, 'state_department': 156, 'russia': 152, 'campaign_finance': 148, 'fiscal_cliff': 137, 'isis': 128, 'great_britain': 127, 'civil_rights': 127, 'holidays': 110, 'china': 110, 'sexual_misconduct': 108, 'national_defense': 106, 'labor': 103, 'europe': 101, 'israel': 97, 'homeland_security': 93, 'criminal_justice': 90, 'disaster': 87, 'marijuana_legalization': 82, 'voting_rights_and_voter_fraud': 79, 'cybersecurity': 78, 'mexico': 78, 'business': 70, 'sports': 70, 'ebola': 67, 'facts_and_fact_checking': 64, 'energy': 63, 'bridging_divides': 61, 'transportation': 58, 'fake_news': 56, 'veterans_affairs': 56, 'treasury': 55, \"women's_issues\": 52, 'asia': 52, 'role_of_government': 46, 'cia': 45, 'us_constitution': 45, 'tea_party': 41, 'privacy': 40, 'arts_and_entertainment': 40, 'campaign_rhetoric': 36, 'epa': 33, 'inequality': 31, 'nuclear_weapons': 30, 'media_industry': 29, 'opioid_crisis': 27, 'palestine': 27, 'agriculture': 24, 'science': 21, 'housing_and_homelessness': 19, 'domestic_policy': 18, 'food': 17, 'death_penalty': 16, 'social_security': 15, 'public_health': 14, 'animal_welfare': 13, 'welfare': 13, 'africa': 11, 'family_and_marriage': 11, 'south_korea': 9, 'fda': 8, 'obesity_and_malnutrition': 8, 'capital_punishment_and_death_penalty': 6, 'dea': 4})\n"
     ]
    }
   ],
   "source": [
    "# Explore topics and sources\n",
    "topics = Counter(article[\"topic\"] for article in random_train_articles)\n",
    "print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvABfnx2KWQA"
   },
   "outputs": [],
   "source": [
    "# Tokenize and preprocess content\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Advanced TF-IDF with n-grams and stopwords\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words = \"english\")\n",
    "X = [article[\"content\"] for article in random_train_articles]\n",
    "X_vec = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wl2h8cpoR6aU",
    "outputId": "1c1ef8fa-aa77-4d31-95d5-817edb5029da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['topic', 'source', 'bias', 'url', 'title', 'date', 'authors', 'content',\n",
      "       'content_original', 'source_url', 'bias_text', 'ID', 'source_encoded'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKfOqfQtwO-9"
   },
   "source": [
    "##**With extra features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcHq0q4HTZE2"
   },
   "source": [
    "### Logistic Regression #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LjVh23vaJzCx",
    "outputId": "f35453b6-29fd-4223-da55-59df11c809bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.80      0.81      2438\n",
      "           1       0.80      0.75      0.77      1976\n",
      "           2       0.79      0.84      0.81      2560\n",
      "\n",
      "    accuracy                           0.80      6974\n",
      "   macro avg       0.80      0.80      0.80      6974\n",
      "weighted avg       0.80      0.80      0.80      6974\n",
      "\n",
      "Test Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.82      0.81       402\n",
      "           1       0.75      0.82      0.78       298\n",
      "           2       0.86      0.81      0.83       599\n",
      "\n",
      "    accuracy                           0.81      1299\n",
      "   macro avg       0.80      0.82      0.81      1299\n",
      "weighted avg       0.82      0.81      0.81      1299\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, Normalizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "\n",
    "# Convert lists into DataFrames\n",
    "df_train = pd.DataFrame(random_train_articles)\n",
    "df_val = pd.DataFrame(random_val_articles)\n",
    "df_test = pd.DataFrame(random_test_articles)\n",
    "\n",
    "# Drop problematic sources\n",
    "drop_list = {\"Guest Writer\", \"Yellow Scene Magazine\", \"Aaron Carroll\", \"Ken Burns\",\n",
    "             \"Carol Costello\", \"The Intelligencer\", \"Polish Times\"}\n",
    "df_train = df_train[~df_train[\"source\"].isin(drop_list)]\n",
    "df_val = df_val[~df_val[\"source\"].isin(drop_list)]\n",
    "df_test = df_test[~df_test[\"source\"].isin(drop_list)]\n",
    "\n",
    "# Encode 'source' as categorical, handling unseen labels\n",
    "label_encoder = LabelEncoder()\n",
    "df_train[\"source_encoded\"] = label_encoder.fit_transform(df_train[\"source\"])\n",
    "# Handle unseen labels in validation and test sets\n",
    "df_val[\"source_encoded\"] = df_val[\"source\"].apply(lambda x: label_encoder.transform([x])[0] if x in label_encoder.classes_ else -1)\n",
    "df_test[\"source_encoded\"] = df_test[\"source\"].apply(lambda x: label_encoder.transform([x])[0] if x in label_encoder.classes_ else -1)\n",
    "\n",
    "# Assign bias scores using allsides_media_bias_ratings\n",
    "df_train[\"source_bias\"] = df_train[\"source\"].map(allsides_media_bias_ratings).fillna(1)  # Default neutral bias\n",
    "df_val[\"source_bias\"] = df_val[\"source\"].map(allsides_media_bias_ratings).fillna(1)\n",
    "df_test[\"source_bias\"] = df_test[\"source\"].map(allsides_media_bias_ratings).fillna(1)\n",
    "\n",
    "# Define features & labels\n",
    "X_train_text = df_train[\"content\"]\n",
    "X_val_text = df_val[\"content\"]\n",
    "X_test_text = df_test[\"content\"]\n",
    "\n",
    "X_train_extra = df_train[[\"source_encoded\", \"source_bias\"]]\n",
    "X_val_extra = df_val[[\"source_encoded\", \"source_bias\"]]\n",
    "X_test_extra = df_test[[\"source_encoded\", \"source_bias\"]]\n",
    "\n",
    "y_train = df_train[\"bias\"]\n",
    "y_val = df_val[\"bias\"]\n",
    "y_test = df_test[\"bias\"]\n",
    "\n",
    "bias_counts = Counter(y_train)\n",
    "\n",
    "# Optimized SMOTE: Ensure oversampling, not downsampling\n",
    "sampling_strategy = {\n",
    "    0: max(bias_counts[0], 3000),  # Use 'max' to ensure oversampling\n",
    "    1: max(bias_counts[1], 3000),  # Use 'max' to ensure oversampling\n",
    "    2: max(bias_counts[2], 3000),  # Use 'max' to ensure oversampling\n",
    "}\n",
    "\n",
    "# Improved TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 3), stop_words=\"english\",\n",
    "    max_features=50000, min_df=2, max_df=0.95,\n",
    "    sublinear_tf=True, analyzer=\"word\"\n",
    ")\n",
    "X_train_vec = vectorizer.fit_transform(X_train_text)\n",
    "X_val_vec = vectorizer.transform(X_val_text)\n",
    "X_test_vec = vectorizer.transform(X_test_text)\n",
    "\n",
    "# Normalize extra features\n",
    "normalizer = Normalizer()\n",
    "X_train_extra = normalizer.fit_transform(X_train_extra)\n",
    "X_val_extra = normalizer.transform(X_val_extra)\n",
    "X_test_extra = normalizer.transform(X_test_extra)\n",
    "\n",
    "# Concatenate TF-IDF vectors with extra features\n",
    "X_train_final = hstack((X_train_vec, X_train_extra))\n",
    "X_val_final = hstack((X_val_vec, X_val_extra))\n",
    "X_test_final = hstack((X_test_vec, X_test_extra))\n",
    "\n",
    "# Optimized SMOTE to avoid excessive oversampling\n",
    "smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
    "X_train_final, y_train = smote.fit_resample(X_train_final, y_train)\n",
    "\n",
    "# Train optimized Logistic Regression\n",
    "model = LogisticRegression(solver=\"saga\", max_iter=1000, C=3.0, penalty='l2')\n",
    "model.fit(X_train_final, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_val_pred = model.predict(X_val_final)\n",
    "y_test_pred = model.predict(X_test_final)\n",
    "\n",
    "print(\"Validation Set Performance:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "print(\"Test Set Performance:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8UboIwL4dWla",
    "outputId": "d6e60b2e-d165-4082-f01f-703f3338c511"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-6c832f6a40fd>:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"source_encoded\"] = label_encoder.fit_transform(df_train[\"source\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.80      0.80      2438\n",
      "           1       0.80      0.75      0.77      1976\n",
      "           2       0.79      0.84      0.81      2560\n",
      "\n",
      "    accuracy                           0.80      6974\n",
      "   macro avg       0.80      0.80      0.80      6974\n",
      "weighted avg       0.80      0.80      0.80      6974\n",
      "\n",
      "Test Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.82      0.81       402\n",
      "           1       0.75      0.82      0.78       298\n",
      "           2       0.86      0.81      0.83       599\n",
      "\n",
      "    accuracy                           0.81      1299\n",
      "   macro avg       0.80      0.82      0.81      1299\n",
      "weighted avg       0.82      0.81      0.82      1299\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, Normalizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "\n",
    "# Convert lists into DataFrames\n",
    "df_train = pd.DataFrame(random_train_articles)\n",
    "df_val = pd.DataFrame(random_val_articles)\n",
    "df_test = pd.DataFrame(random_test_articles)\n",
    "\n",
    "# Drop problematic sources\n",
    "drop_list = {\"Guest Writer\", \"Yellow Scene Magazine\", \"Aaron Carroll\", \"Ken Burns\",\n",
    "             \"Carol Costello\", \"The Intelligencer\", \"Polish Times\"}\n",
    "df_train = df_train[~df_train[\"source\"].isin(drop_list)]\n",
    "df_val = df_val[~df_val[\"source\"].isin(drop_list)]\n",
    "df_test = df_test[~df_test[\"source\"].isin(drop_list)]\n",
    "\n",
    "# Encode 'source' as categorical, handling unseen labels\n",
    "label_encoder = LabelEncoder()\n",
    "df_train[\"source_encoded\"] = label_encoder.fit_transform(df_train[\"source\"])\n",
    "# Handle unseen labels in validation and test sets\n",
    "df_val[\"source_encoded\"] = df_val[\"source\"].apply(lambda x: label_encoder.transform([x])[0] if x in label_encoder.classes_ else -1)\n",
    "df_test[\"source_encoded\"] = df_test[\"source\"].apply(lambda x: label_encoder.transform([x])[0] if x in label_encoder.classes_ else -1)\n",
    "\n",
    "# Assign bias scores using allsides_media_bias_ratings\n",
    "df_train[\"source_bias\"] = df_train[\"source\"].map(allsides_media_bias_ratings).fillna(1)  # Default neutral bias\n",
    "df_val[\"source_bias\"] = df_val[\"source\"].map(allsides_media_bias_ratings).fillna(1)\n",
    "df_test[\"source_bias\"] = df_test[\"source\"].map(allsides_media_bias_ratings).fillna(1)\n",
    "\n",
    "# Define features & labels\n",
    "X_train_text = df_train[\"content\"]\n",
    "X_val_text = df_val[\"content\"]\n",
    "X_test_text = df_test[\"content\"]\n",
    "\n",
    "X_train_extra = df_train[[\"source_encoded\", \"source_bias\"]]\n",
    "X_val_extra = df_val[[\"source_encoded\", \"source_bias\"]]\n",
    "X_test_extra = df_test[[\"source_encoded\", \"source_bias\"]]\n",
    "\n",
    "y_train = df_train[\"bias\"]\n",
    "y_val = df_val[\"bias\"]\n",
    "y_test = df_test[\"bias\"]\n",
    "\n",
    "bias_counts = Counter(y_train)\n",
    "\n",
    "# Optimized SMOTE: Ensure oversampling, not downsampling\n",
    "sampling_strategy = {\n",
    "    0: max(bias_counts[0], 3000),  # Use 'max' to ensure oversampling\n",
    "    1: max(bias_counts[1], 3000),  # Use 'max' to ensure oversampling\n",
    "    2: max(bias_counts[2], 3000),  # Use 'max' to ensure oversampling\n",
    "}\n",
    "\n",
    "# Improved TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),  # Use only unigrams and bigrams\n",
    "    stop_words=\"english\",\n",
    "    max_features=50000,  # Reduce to 50K features\n",
    "    min_df=10,  # Filter out rare words\n",
    "    max_df=0.8,  # Exclude overly common words\n",
    "    sublinear_tf=True\n",
    "  )\n",
    "\n",
    "X_train_vec = vectorizer.fit_transform(X_train_text)\n",
    "X_val_vec = vectorizer.transform(X_val_text)\n",
    "X_test_vec = vectorizer.transform(X_test_text)\n",
    "\n",
    "# Normalize extra features\n",
    "normalizer = Normalizer()\n",
    "X_train_extra = normalizer.fit_transform(X_train_extra)\n",
    "X_val_extra = normalizer.transform(X_val_extra)\n",
    "X_test_extra = normalizer.transform(X_test_extra)\n",
    "\n",
    "# Concatenate TF-IDF vectors with extra features\n",
    "X_train_final = hstack((X_train_vec, X_train_extra))\n",
    "X_val_final = hstack((X_val_vec, X_val_extra))\n",
    "X_test_final = hstack((X_test_vec, X_test_extra))\n",
    "\n",
    "# Optimized SMOTE to avoid excessive oversampling\n",
    "smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
    "X_train_final, y_train = smote.fit_resample(X_train_final, y_train)\n",
    "\n",
    "# Train optimized Logistic Regression\n",
    "model = LogisticRegression(solver=\"saga\", max_iter=1000, C=3.0, penalty='l2')\n",
    "model.fit(X_train_final, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_val_pred = model.predict(X_val_final)\n",
    "y_test_pred = model.predict(X_test_final)\n",
    "\n",
    "print(\"Validation Set Performance:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "print(\"Test Set Performance:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dH7jAVI4An-3"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the vectorizer\n",
    "assert(type(vectorizer) == TfidfVectorizer)\n",
    "\n",
    "# Save the vectorizer to a file\n",
    "with open('/content/gdrive/MyDrive/Capstone Project 2025/Models/tfidf_vectorizer.pkl', \"wb\") as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "# Save the normalizer\n",
    "with open('/content/gdrive/MyDrive/Capstone Project 2025/Models/normalizer.pkl', \"wb\") as f:\n",
    "    pickle.dump(normalizer, f)\n",
    "\n",
    "# Save the logistic regression model\n",
    "with open('/content/gdrive/MyDrive/Capstone Project 2025/Models/logreg.pkl', \"wb\") as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74Uf-9dWUHH-"
   },
   "source": [
    "### Logistic Regression #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RkfcUzy3O6Io",
    "outputId": "7df6c933-51c9-4db4-8620-d931b55ae919"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Best Parameters: {'C': 20, 'class_weight': None, 'max_iter': 3000, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Validation Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.90      0.90      7800\n",
      "           1       0.91      0.85      0.88      6390\n",
      "           2       0.88      0.91      0.90      8192\n",
      "\n",
      "    accuracy                           0.89     22382\n",
      "   macro avg       0.89      0.89      0.89     22382\n",
      "weighted avg       0.89      0.89      0.89     22382\n",
      "\n",
      "Final Test Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.76      0.76      1950\n",
      "           1       0.78      0.71      0.75      1598\n",
      "           2       0.74      0.79      0.76      2048\n",
      "\n",
      "    accuracy                           0.76      5596\n",
      "   macro avg       0.76      0.75      0.76      5596\n",
      "weighted avg       0.76      0.76      0.76      5596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Train-Test Split (Keep test_size=0.2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Keep k_best at 10,000\n",
    "k_best = SelectKBest(f_classif, k=10000)\n",
    "X_train_selected = k_best.fit_transform(X_train, y_train)\n",
    "X_test_selected = k_best.transform(X_test)\n",
    "\n",
    "# Grid Search Parameters\n",
    "param_grid = {\n",
    "    'C': [5, 10, 20],  # Increase regularization strength\n",
    "    'penalty': ['l1', 'l2'],  # Test both types of regularization\n",
    "    'solver': ['liblinear'],  # Best for smaller datasets\n",
    "    'max_iter': [3000],  # Increase iterations for better convergence\n",
    "    'class_weight': [None]  # Remove 'balanced' to restore proportions\n",
    "}\n",
    "\n",
    "# Train Model\n",
    "logreg = LogisticRegression()\n",
    "grid_search = GridSearchCV(logreg, param_grid, cv=3, scoring='accuracy', n_jobs=4, verbose=2)\n",
    "grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "# Best Parameters\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Validation Performance\n",
    "y_val_pred = grid_search.predict(X_train_selected)\n",
    "print(\"Validation Performance:\")\n",
    "print(classification_report(y_train, y_val_pred))\n",
    "\n",
    "# Test Performance\n",
    "y_test_pred = grid_search.predict(X_test_selected)\n",
    "print(\"Final Test Performance:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUcMmTR8UUJC"
   },
   "source": [
    "### Logistic Regression #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L_-W0LG9sa_d",
    "outputId": "331c6b95-9233-4ce9-8c13-45e999c8570a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.77      0.78      2438\n",
      "           1       0.77      0.75      0.76      1976\n",
      "           2       0.76      0.80      0.78      2560\n",
      "\n",
      "    accuracy                           0.77      6974\n",
      "   macro avg       0.78      0.77      0.77      6974\n",
      "weighted avg       0.78      0.77      0.77      6974\n",
      "\n",
      "Test Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.73      0.72       402\n",
      "           1       0.66      0.85      0.74       298\n",
      "           2       0.85      0.70      0.76       599\n",
      "\n",
      "    accuracy                           0.74      1299\n",
      "   macro avg       0.74      0.76      0.74      1299\n",
      "weighted avg       0.76      0.74      0.74      1299\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, Normalizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "\n",
    "# Convert lists into DataFrames\n",
    "df_train = pd.DataFrame(random_train_articles)\n",
    "df_val = pd.DataFrame(random_val_articles)\n",
    "df_test = pd.DataFrame(random_test_articles)\n",
    "\n",
    "# Drop problematic sources\n",
    "drop_list = {\"Guest Writer\", \"Yellow Scene Magazine\", \"Aaron Carroll\", \"Ken Burns\",\n",
    "             \"Carol Costello\", \"The Intelligencer\", \"Polish Times\"}\n",
    "df_train = df_train[~df_train[\"source\"].isin(drop_list)]\n",
    "df_val = df_val[~df_val[\"source\"].isin(drop_list)]\n",
    "df_test = df_test[~df_test[\"source\"].isin(drop_list)]\n",
    "\n",
    "# Encode 'source' as categorical, handling unseen labels\n",
    "label_encoder = LabelEncoder()\n",
    "df_train[\"source_encoded\"] = label_encoder.fit_transform(df_train[\"source\"])\n",
    "# Handle unseen labels in validation and test sets\n",
    "df_val[\"source_encoded\"] = df_val[\"source\"].apply(lambda x: label_encoder.transform([x])[0] if x in label_encoder.classes_ else -1)\n",
    "df_test[\"source_encoded\"] = df_test[\"source\"].apply(lambda x: label_encoder.transform([x])[0] if x in label_encoder.classes_ else -1)\n",
    "\n",
    "# Assign bias scores using allsides_media_bias_ratings\n",
    "df_train[\"source_bias\"] = df_train[\"source\"].map(allsides_media_bias_ratings).fillna(1)  # Default neutral bias\n",
    "df_val[\"source_bias\"] = df_val[\"source\"].map(allsides_media_bias_ratings).fillna(1)\n",
    "df_test[\"source_bias\"] = df_test[\"source\"].map(allsides_media_bias_ratings).fillna(1)\n",
    "\n",
    "# Define features & labels\n",
    "X_train_text = df_train[\"content\"]\n",
    "X_val_text = df_val[\"content\"]\n",
    "X_test_text = df_test[\"content\"]\n",
    "\n",
    "X_train_extra = df_train[[\"source_encoded\", \"source_bias\"]]\n",
    "X_val_extra = df_val[[\"source_encoded\", \"source_bias\"]]\n",
    "X_test_extra = df_test[[\"source_encoded\", \"source_bias\"]]\n",
    "\n",
    "y_train = df_train[\"bias\"]\n",
    "y_val = df_val[\"bias\"]\n",
    "y_test = df_test[\"bias\"]\n",
    "\n",
    "bias_counts = Counter(y_train)\n",
    "\n",
    "# Improved TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 4), stop_words=\"english\",\n",
    "    max_features=150000, min_df=5, max_df=0.88,\n",
    "    sublinear_tf=True, analyzer=\"word\"\n",
    ")\n",
    "X_train_vec = vectorizer.fit_transform(X_train_text)\n",
    "X_val_vec = vectorizer.transform(X_val_text)\n",
    "X_test_vec = vectorizer.transform(X_test_text)\n",
    "\n",
    "# SMOTE Strategy with a Higher Oversampling Limit\n",
    "bias_counts = Counter(y_train)\n",
    "sampling_strategy = {\n",
    "    0: max(bias_counts[0], 7000),  # Use max to ensure oversampling\n",
    "    1: max(bias_counts[1], 7000),  # Use max to ensure oversampling\n",
    "    2: max(bias_counts[2], 7000),  # Use max to ensure oversampling\n",
    "}\n",
    "\n",
    "smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42, k_neighbors=3)\n",
    "X_train_final, y_train = smote.fit_resample(X_train_vec, y_train)\n",
    "\n",
    "# Optimized Logistic Regression Model\n",
    "model = LogisticRegression(solver=\"saga\", max_iter=1500, C=7.0, penalty=\"elasticnet\", l1_ratio=0.7)\n",
    "model.fit(X_train_final, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_val_pred = model.predict(X_val_vec)\n",
    "y_test_pred = model.predict(X_test_vec)\n",
    "\n",
    "# Performance Report\n",
    "print(\"Validation Set Performance:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "print(\"Test Set Performance:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uy8JN8OCyar1"
   },
   "source": [
    "### Random Forest #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VygAcgiJ15t1",
    "outputId": "701b68ce-06d8-43a8-ba1a-6eced5d6fd58"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-828c8e29e9fd>:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"source_encoded\"] = label_encoder.fit_transform(df_train[\"source\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.85      0.86      2438\n",
      "           1       0.90      0.73      0.80      1976\n",
      "           2       0.84      0.98      0.90      2560\n",
      "\n",
      "    accuracy                           0.86      6974\n",
      "   macro avg       0.87      0.85      0.85      6974\n",
      "weighted avg       0.87      0.86      0.86      6974\n",
      "\n",
      "Test Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.65      0.76       402\n",
      "           1       0.89      0.77      0.82       298\n",
      "           2       0.77      0.97      0.86       599\n",
      "\n",
      "    accuracy                           0.82      1299\n",
      "   macro avg       0.85      0.79      0.81      1299\n",
      "weighted avg       0.84      0.82      0.82      1299\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, Normalizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Convert lists into DataFrames\n",
    "df_train = pd.DataFrame(random_train_articles)\n",
    "df_val = pd.DataFrame(random_val_articles)\n",
    "df_test = pd.DataFrame(random_test_articles)\n",
    "\n",
    "# Drop problematic sources\n",
    "drop_list = {\"Guest Writer\", \"Yellow Scene Magazine\", \"Aaron Carroll\", \"Ken Burns\",\n",
    "             \"Carol Costello\", \"The Intelligencer\", \"Polish Times\"}\n",
    "df_train = df_train.loc[~df_train[\"source\"].isin(drop_list)]\n",
    "df_val = df_val.loc[~df_val[\"source\"].isin(drop_list)]\n",
    "df_test = df_test.loc[~df_test[\"source\"].isin(drop_list)]\n",
    "\n",
    "# Encode 'source' as categorical\n",
    "label_encoder = LabelEncoder()\n",
    "df_train[\"source_encoded\"] = label_encoder.fit_transform(df_train[\"source\"])\n",
    "df_val[\"source_encoded\"] = df_val[\"source\"].map(lambda x: label_encoder.transform([x])[0] if x in label_encoder.classes_ else -1)\n",
    "df_test[\"source_encoded\"] = df_test[\"source\"].map(lambda x: label_encoder.transform([x])[0] if x in label_encoder.classes_ else -1)\n",
    "\n",
    "# Assign bias scores\n",
    "df_train[\"source_bias\"] = df_train[\"source\"].map(allsides_media_bias_ratings).fillna(1)\n",
    "df_val[\"source_bias\"] = df_val[\"source\"].map(allsides_media_bias_ratings).fillna(1)\n",
    "df_test[\"source_bias\"] = df_test[\"source\"].map(allsides_media_bias_ratings).fillna(1)\n",
    "\n",
    "# Define features & labels\n",
    "X_train_text = df_train[\"content\"]\n",
    "X_val_text = df_val[\"content\"]\n",
    "X_test_text = df_test[\"content\"]\n",
    "\n",
    "X_train_extra = df_train[[\"source_encoded\", \"source_bias\"]]\n",
    "X_val_extra = df_val[[\"source_encoded\", \"source_bias\"]]\n",
    "X_test_extra = df_test[[\"source_encoded\", \"source_bias\"]]\n",
    "\n",
    "y_train = df_train[\"bias\"]\n",
    "y_val = df_val[\"bias\"]\n",
    "y_test = df_test[\"bias\"]\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 4), stop_words=\"english\",\n",
    "    max_features=60000, sublinear_tf=True,\n",
    "    analyzer=\"word\"\n",
    ")\n",
    "X_train_vec = vectorizer.fit_transform(X_train_text)\n",
    "X_val_vec = vectorizer.transform(X_val_text)\n",
    "X_test_vec = vectorizer.transform(X_test_text)\n",
    "\n",
    "# Normalize extra features\n",
    "normalizer = Normalizer()\n",
    "X_train_extra = normalizer.fit_transform(X_train_extra)\n",
    "X_val_extra = normalizer.transform(X_val_extra)\n",
    "X_test_extra = normalizer.transform(X_test_extra)\n",
    "\n",
    "# Concatenate TF-IDF vectors with extra features\n",
    "X_train_final = hstack((X_train_vec, X_train_extra))\n",
    "X_val_final = hstack((X_val_vec, X_val_extra))\n",
    "X_test_final = hstack((X_test_vec, X_test_extra))\n",
    "\n",
    "# Apply SMOTETomek for better class balancing\n",
    "smote_tomek = SMOTETomek(random_state=42)\n",
    "X_train_final, y_train = smote_tomek.fit_resample(X_train_final, y_train)\n",
    "\n",
    "# Train Random Forest Classifier\n",
    "model = RandomForestClassifier(n_estimators=400, max_depth=55, random_state=42)\n",
    "model.fit(X_train_final, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_val_pred = model.predict(X_val_final)\n",
    "y_test_pred = model.predict(X_test_final)\n",
    "\n",
    "print(\"Validation Set Performance:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "print(\"Test Set Performance:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kfg6LGMEY_HM"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the vectorizer\n",
    "assert(type(vectorizer) == TfidfVectorizer)\n",
    "\n",
    "# Save the scaler\n",
    "with open('/content/gdrive/MyDrive/Capstone Project 2025/Models/rf_normalizer.pkl', \"wb\") as f:\n",
    "    pickle.dump(normalizer, f)\n",
    "\n",
    "# Save the Random Forest\n",
    "with open('/content/gdrive/MyDrive/Capstone Project 2025/Models/rf.pkl', \"wb\") as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-fzHDPuYZ-hJ",
    "outputId": "8df7c94e-5d19-49e0-ccbf-617a006d62b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 55, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 400, 'n_jobs': None, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# Print SGD's parameters\n",
    "print(model.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSNxwLcUypJV"
   },
   "source": [
    "### Random Forest #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bJwmrw4SqRnf",
    "outputId": "9b30fc0b-4ba7-4c7c-aa53-59e72031bc83"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:539: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.78      0.82      2438\n",
      "           1       0.80      0.83      0.81      1976\n",
      "           2       0.80      0.87      0.84      2560\n",
      "\n",
      "    accuracy                           0.83      6974\n",
      "   macro avg       0.83      0.82      0.82      6974\n",
      "weighted avg       0.83      0.83      0.83      6974\n",
      "\n",
      "Test Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.55      0.64       402\n",
      "           1       0.68      0.88      0.76       298\n",
      "           2       0.78      0.81      0.79       599\n",
      "\n",
      "    accuracy                           0.74      1299\n",
      "   macro avg       0.74      0.75      0.73      1299\n",
      "weighted avg       0.75      0.74      0.74      1299\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Vectorization with optimizations\n",
    "from sklearn.ensemble import RandomForestClassifier # This line is added to import RandomForestClassifier\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 3),  # Reduced to trigrams\n",
    "    stop_words=\"english\",\n",
    "    max_features=30000,  # Reduced feature space to prevent memory overload\n",
    "    sublinear_tf=True,\n",
    "    analyzer=\"char_wb\"  # Character-level analysis adds feature diversity\n",
    ")\n",
    "X_train_vec = vectorizer.fit_transform(X_train_text)\n",
    "X_val_vec = vectorizer.transform(X_val_text)\n",
    "X_test_vec = vectorizer.transform(X_test_text)\n",
    "\n",
    "# Normalize extra features\n",
    "normalizer = Normalizer()\n",
    "X_train_extra = normalizer.fit_transform(X_train_extra)\n",
    "X_val_extra = normalizer.transform(X_val_extra)\n",
    "X_test_extra = normalizer.transform(X_test_extra)\n",
    "\n",
    "# Concatenate TF-IDF vectors with extra features\n",
    "X_train_final = hstack((X_train_vec, X_train_extra))\n",
    "X_val_final = hstack((X_val_vec, X_val_extra))\n",
    "X_test_final = hstack((X_test_vec, X_test_extra))\n",
    "\n",
    "# Class balancing with SMOTE + Tomek Links (reduced over-balancing)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "# Get the original class distribution\n",
    "from collections import Counter\n",
    "original_class_distribution = Counter(y_train)\n",
    "\n",
    "# Set sampling strategy to at least the original size or a target size\n",
    "sampling_strategy = {\n",
    "    class_label: max(count, 2000)  # Oversample to at least 2000 if the original size is smaller\n",
    "    for class_label, count in original_class_distribution.items()\n",
    "}\n",
    "\n",
    "smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_final, y_train)\n",
    "\n",
    "tomek = TomekLinks()\n",
    "X_train_final, y_train = tomek.fit_resample(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Train Random Forest Classifier with better generalization\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=200,  # Reduced number of trees for efficiency\n",
    "    max_depth=30,  # Shallower trees prevent overfitting\n",
    "    min_samples_split=5,  # Requires more samples per split\n",
    "    min_samples_leaf=3,  # Prevents overfitting small details\n",
    "    max_features=\"sqrt\",  # Introduces randomness\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train_final, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_val_pred = model.predict(X_val_final)\n",
    "y_test_pred = model.predict(X_test_final)\n",
    "\n",
    "print(\"Validation Set Performance:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "print(\"Test Set Performance:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDNk7fOly1KW"
   },
   "source": [
    "### XGBoost Classifier #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9LdI8c3LSSQd",
    "outputId": "7a103f3b-27f7-496e-fd45-cb9bac9bd110"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [14:51:18] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      2438\n",
      "           1       0.99      0.99      0.99      1976\n",
      "           2       0.99      0.99      0.99      2560\n",
      "\n",
      "    accuracy                           0.99      6974\n",
      "   macro avg       0.99      0.99      0.99      6974\n",
      "weighted avg       0.99      0.99      0.99      6974\n",
      "\n",
      "Test Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       402\n",
      "           1       1.00      0.99      0.99       298\n",
      "           2       1.00      0.98      0.99       599\n",
      "\n",
      "    accuracy                           0.99      1299\n",
      "   macro avg       0.99      0.99      0.99      1299\n",
      "weighted avg       0.99      0.99      0.99      1299\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import gc\n",
    "\n",
    "# Free up memory before training\n",
    "gc.collect()\n",
    "\n",
    "# XGBoost Model (Memory Optimized)\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=200,  # Keep reasonable tree count\n",
    "    max_depth=6,  # Limit tree depth to prevent overfitting & large memory usage\n",
    "    learning_rate=0.05,  # Small step size for better generalization\n",
    "    subsample=0.8,  # Uses 80% of data per tree to save memory\n",
    "    colsample_bytree=0.8,  # Reduces memory usage\n",
    "    eval_metric=\"logloss\",  # Better performance on classification\n",
    "    tree_method=\"hist\",  # Optimized for large datasets\n",
    "    use_label_encoder=False,  # Avoids warnings\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train model\n",
    "xgb_model.fit(X_train_final, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_val_pred = xgb_model.predict(X_val_final)\n",
    "y_test_pred = xgb_model.predict(X_test_final)\n",
    "\n",
    "# Evaluate performance\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"Validation Set Performance:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "print(\"Test Set Performance:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGGpfa3Ay8UK"
   },
   "source": [
    "### SGDC Classifier #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "biud4fAq08cu",
    "outputId": "f7c86180-78f9-4e39-c8dc-b2b4420a1b6d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-c57b045b2c23>:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train.loc[:, \"source_encoded\"] = label_encoder.fit_transform(df_train[\"source\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.92      2438\n",
      "           1       0.92      0.81      0.86      1976\n",
      "           2       0.97      0.99      0.98      2560\n",
      "\n",
      "    accuracy                           0.92      6974\n",
      "   macro avg       0.92      0.92      0.92      6974\n",
      "weighted avg       0.93      0.92      0.92      6974\n",
      "\n",
      "Test Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.98      0.88       402\n",
      "           1       0.88      0.67      0.76       298\n",
      "           2       0.99      0.97      0.98       599\n",
      "\n",
      "    accuracy                           0.90      1299\n",
      "   macro avg       0.89      0.87      0.88      1299\n",
      "weighted avg       0.91      0.90      0.90      1299\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "\n",
    "# Convert lists into DataFrames\n",
    "df_train = pd.DataFrame(random_train_articles)\n",
    "df_val = pd.DataFrame(random_val_articles)\n",
    "df_test = pd.DataFrame(random_test_articles)\n",
    "\n",
    "# Drop problematic sources\n",
    "drop_list = {\"Guest Writer\", \"Yellow Scene Magazine\", \"Aaron Carroll\", \"Ken Burns\",\n",
    "             \"Carol Costello\", \"The Intelligencer\", \"Polish Times\"}\n",
    "df_train = df_train.loc[~df_train[\"source\"].isin(drop_list)]\n",
    "df_val = df_val.loc[~df_val[\"source\"].isin(drop_list)]\n",
    "df_test = df_test.loc[~df_test[\"source\"].isin(drop_list)]\n",
    "\n",
    "# Encode 'source' as categorical, handling unseen labels\n",
    "label_encoder = LabelEncoder()\n",
    "df_train.loc[:, \"source_encoded\"] = label_encoder.fit_transform(df_train[\"source\"])\n",
    "df_val.loc[:, \"source_encoded\"] = df_val[\"source\"].apply(lambda x: label_encoder.transform([x])[0] if x in label_encoder.classes_ else -1)\n",
    "df_test.loc[:, \"source_encoded\"] = df_test[\"source\"].apply(lambda x: label_encoder.transform([x])[0] if x in label_encoder.classes_ else -1)\n",
    "\n",
    "# Assign bias scores using allsides_media_bias_ratings\n",
    "df_train.loc[:, \"source_bias\"] = df_train[\"source\"].map(allsides_media_bias_ratings).fillna(1)\n",
    "df_val.loc[:, \"source_bias\"] = df_val[\"source\"].map(allsides_media_bias_ratings).fillna(1)\n",
    "df_test.loc[:, \"source_bias\"] = df_test[\"source\"].map(allsides_media_bias_ratings).fillna(1)\n",
    "\n",
    "# Define features & labels\n",
    "X_train_text = df_train[\"content\"]\n",
    "X_val_text = df_val[\"content\"]\n",
    "X_test_text = df_test[\"content\"]\n",
    "\n",
    "X_train_extra = df_train[[\"source_encoded\", \"source_bias\"]]\n",
    "X_val_extra = df_val[[\"source_encoded\", \"source_bias\"]]\n",
    "X_test_extra = df_test[[\"source_encoded\", \"source_bias\"]]\n",
    "\n",
    "y_train = df_train[\"bias\"]\n",
    "y_val = df_val[\"bias\"]\n",
    "y_test = df_test[\"bias\"]\n",
    "\n",
    "# Normalize source features\n",
    "scaler = MinMaxScaler()\n",
    "X_train_extra = scaler.fit_transform(X_train_extra)\n",
    "X_val_extra = scaler.transform(X_val_extra)\n",
    "X_test_extra = scaler.transform(X_test_extra)\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),  # Use only unigrams and bigrams\n",
    "    stop_words=\"english\",\n",
    "    max_features=50000,  # Reduce to 50K features\n",
    "    min_df=10,  # Filter out rare words\n",
    "    max_df=0.8,  # Exclude overly common words\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "X_train_vec = vectorizer.fit_transform(X_train_text)\n",
    "X_val_vec = vectorizer.transform(X_val_text)\n",
    "X_test_vec = vectorizer.transform(X_test_text)\n",
    "\n",
    "# Reduce precision to save RAM\n",
    "X_train_vec = X_train_vec.astype(\"float32\")\n",
    "X_val_vec = X_val_vec.astype(\"float32\")\n",
    "X_test_vec = X_test_vec.astype(\"float32\")\n",
    "\n",
    "# Combine Text and Source Features\n",
    "X_train_combined = hstack([X_train_vec, X_train_extra])\n",
    "X_val_combined = hstack([X_val_vec, X_val_extra])\n",
    "X_test_combined = hstack([X_test_vec, X_test_extra])\n",
    "\n",
    "# SMOTE with Lower Sampling\n",
    "bias_counts = Counter(y_train)\n",
    "# Limit oversampling to avoid RAM issues\n",
    "smote = SMOTE(sampling_strategy=\"auto\", random_state=42, k_neighbors=3)\n",
    "# Apply SMOTE to the combined features to ensure consistency.\n",
    "X_train_final, y_train = smote.fit_resample(X_train_combined, y_train)\n",
    "\n",
    "# Train SGDC for Higher Accuracy\n",
    "model = SGDClassifier(loss=\"log_loss\", max_iter=1000, alpha=1e-4, penalty=\"l2\", random_state=42)\n",
    "\n",
    "model.fit(X_train_final, y_train)\n",
    "\n",
    "# Predictions\n",
    "# Use X_val_combined and X_test_combined for prediction to maintain consistency.\n",
    "y_val_pred = model.predict(X_val_combined)\n",
    "y_test_pred = model.predict(X_test_combined)\n",
    "\n",
    "# Performance Report\n",
    "print(\"Validation Set Performance:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "print(\"Test Set Performance:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r10zaNOQU3hY"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the vectorizer\n",
    "assert(type(vectorizer) == TfidfVectorizer)\n",
    "\n",
    "# Save the scaler\n",
    "with open('/content/gdrive/MyDrive/Capstone Project 2025/Models/scaler.pkl', \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Save the SGD model\n",
    "with open('/content/gdrive/MyDrive/Capstone Project 2025/Models/sgd.pkl', \"wb\") as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W23SYtDOy8Qs",
    "outputId": "3d182784-67f9-489e-fef8-32ff2bd6cfbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.0001, 'average': False, 'class_weight': None, 'early_stopping': False, 'epsilon': 0.1, 'eta0': 0.0, 'fit_intercept': True, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'loss': 'log_loss', 'max_iter': 1000, 'n_iter_no_change': 5, 'n_jobs': None, 'penalty': 'l2', 'power_t': 0.5, 'random_state': 42, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# Print SGD's parameters\n",
    "print(model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "V78j8F095Z7T",
    "outputId": "f5560669-de13-4366-ea01-9425a6b2d7d8"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_6b1145f6-b76b-41d6-8904-d1a67fd11bc3\", \"tfidf_sgdc_predictions_extra_features.csv\", 15200196)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.download(\"tfidf_sgdc_predictions_extra_features.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "3Oluody0skGA"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
